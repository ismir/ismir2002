<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0125)file://C:\Documents%20and%20Settings\Fingerhut\Local%20Settings\Temporary%20Internet%20Files\Content.IE5\FBS374OJ\papers.html -->
<!-- saved from url=(0022)http://internet.e-mail -->
<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:w="urn:schemas-microsoft-com:office:word"
xmlns:st1="urn:schemas-microsoft-com:office:smarttags"
xmlns="http://www.w3.org/TR/REC-html40">


<!-- Mirrored from ismir2002.ismir.net/papers.html by HTTrack Website Copier/3.x [XR&CO'2013], Thu, 23 Jan 2014 09:11:05 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=windows-1252" /><!-- /Added by HTTrack -->
<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=ProgId content=Word.Document>
<meta name=Generator content="Microsoft Word 9">
<meta name=Originator content="Microsoft Word 9">
<link rel=File-List href="papers_fichiers/filelist.xml">
<link rel=Edit-Time-Data href="papers_fichiers/editdata.html">
<link rel=OLE-Object-Data href="papers_fichiers/oledata.mso">
<!--[if !mso]>
<style>
v\:* {behavior:url(#default#VML);}
o\:* {behavior:url(#default#VML);}
w\:* {behavior:url(#default#VML);}
.shape {behavior:url(#default#VML);}
</style>
<![endif]-->
<title>ISMIR 2002 Accepted Papers</title>
<!--[if gte mso 9]><xml>
 <o:DocumentProperties>
  <o:Author>Michael Fingerhut</o:Author>
  <o:LastAuthor>Michael Fingerhut</o:LastAuthor>
  <o:Revision>6</o:Revision>
  <o:TotalTime>349</o:TotalTime>
  <o:Created>2003-01-08T11:00:00Z</o:Created>
  <o:LastSaved>2003-01-08T14:07:00Z</o:LastSaved>
  <o:Pages>2</o:Pages>
  <o:Words>1215</o:Words>
  <o:Characters>6927</o:Characters>
  <o:Company>Ircam - Centre Pompidou (C) 2002</o:Company>
  <o:Lines>57</o:Lines>
  <o:Paragraphs>13</o:Paragraphs>
  <o:CharactersWithSpaces>8506</o:CharactersWithSpaces>
  <o:Version>9.4402</o:Version>
 </o:DocumentProperties>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <w:WordDocument>
  <w:Zoom>120</w:Zoom>
  <w:ActiveWritingStyle Lang="EN-GB" VendorID="64" DLLVersion="131078"
   NLCheck="1">1</w:ActiveWritingStyle>
  <w:ActiveWritingStyle Lang="FR" VendorID="64" DLLVersion="131078" NLCheck="1">1</w:ActiveWritingStyle>
  <w:HyphenationZone>21</w:HyphenationZone>
  <w:Compatibility>
   <w:ApplyBreakingRules/>
  </w:Compatibility>
  <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel>
  <w:SpellingState>Clean</w:SpellingState>
  <w:GrammarState>Clean</w:GrammarState>
 </w:WordDocument>
</xml><![endif]--><![if !supportAnnotations]>
<style id="dynCom" type="text/css"><!-- --></style>
<script language="JavaScript"><!--
function msoCommentShow(anchor_id, com_id)
{
	if(msoBrowserCheck()) 
		{
		c = document.all(com_id);
		if (null != c)
			{
			a = document.all(anchor_id);
			var cw = c.offsetWidth;
			var ch = c.offsetHeight;
			var aw = a.offsetWidth;
			var ah = a.offsetHeight;
			var x  = a.offsetLeft;
			var y  = a.offsetTop;
			var el = a;
			while (el.tagName != "BODY") 
				{
				el = el.offsetParent;
				x = x + el.offsetLeft;
				y = y + el.offsetTop;
				}
			var bw = document.body.clientWidth;
			var bh = document.body.clientHeight;
			var bsl = document.body.scrollLeft;
			var bst = document.body.scrollTop;
			if (x + cw + ah / 2 > bw + bsl && x + aw - ah / 2 - cw >= bsl ) 
				{ c.style.left = x + aw - ah / 2 - cw; }
			else 
				{ c.style.left = x + ah / 2; }
			if (y + ch + ah / 2 > bh + bst && y + ah / 2 - ch >= bst ) 
				{ c.style.top = y + ah / 2 - ch; }
			else 
				{ c.style.top = y + ah / 2; }
			c.style.visibility = "visible";
}	}	}
function msoCommentHide(com_id) 
{
	if(msoBrowserCheck())
		{
		c = document.all(com_id);
		if (null != c)
		{
		c.style.visibility = "hidden";
		c.style.left = -1000;
		c.style.top = -1000;
		} } 
}
function msoBrowserCheck()
{
	ms = navigator.appVersion.indexOf("MSIE");
	vers = navigator.appVersion.substring(ms + 5, ms + 6);
	ie4 = (ms > 0) && (parseInt(vers) >= 4);
	return ie4;
}
if (msoBrowserCheck())
{
	document.styleSheets.dynCom.addRule(".msocomanchor","background: infobackground");
	document.styleSheets.dynCom.addRule(".msocomoff","display: none");
	document.styleSheets.dynCom.addRule(".msocomtxt","visibility: hidden");
	document.styleSheets.dynCom.addRule(".msocomtxt","position: absolute");
	document.styleSheets.dynCom.addRule(".msocomtxt","top: -1000");
	document.styleSheets.dynCom.addRule(".msocomtxt","left: -1000");
	document.styleSheets.dynCom.addRule(".msocomtxt","width: 33%");
	document.styleSheets.dynCom.addRule(".msocomtxt","background: infobackground");
	document.styleSheets.dynCom.addRule(".msocomtxt","color: infotext");
	document.styleSheets.dynCom.addRule(".msocomtxt","border-top: 1pt solid threedlightshadow");
	document.styleSheets.dynCom.addRule(".msocomtxt","border-right: 2pt solid threedshadow");
	document.styleSheets.dynCom.addRule(".msocomtxt","border-bottom: 2pt solid threedshadow");
	document.styleSheets.dynCom.addRule(".msocomtxt","border-left: 1pt solid threedlightshadow");
	document.styleSheets.dynCom.addRule(".msocomtxt","padding: 3pt 3pt 3pt 3pt");
}
// --></script>
<![endif]>
<style>
<!--
p.MSOCOMMENTTEXT
	{mso-style-link: "Commentaire Car"
;}
li.MSOCOMMENTTEXT
	{mso-style-link: "Commentaire Car"
;}
div.MSOCOMMENTTEXT
	{mso-style-link: "Commentaire Car"
;}
p.MSOCOMMENTSUBJECT
	{mso-style-noshow: yes
;}
li.MSOCOMMENTSUBJECT
	{mso-style-noshow: yes
;}
div.MSOCOMMENTSUBJECT
	{mso-style-noshow: yes
;}
span.COMMENTAIRECAR
	{mso-style-link: Commentaire;}
span.SPELLE
	{mso-spl-e: yes
;}
span.GRAME
	{mso-gram-e: yes
;}
 /* Font Definitions */
@font-face
	{font-family:"Arial Unicode MS";
	panose-1:2 11 6 4 2 2 2 2 2 4;
	mso-font-charset:128;
	mso-generic-font-family:swiss;
	mso-font-pitch:variable;
	mso-font-signature:-1 -369098753 63 0 4129023 0;}
@font-face
	{font-family:"\@Arial Unicode MS";
	mso-font-charset:128;
	mso-generic-font-family:swiss;
	mso-font-pitch:variable;
	mso-font-signature:-1 -369098753 63 0 4129023 0;}
@font-face
	{font-family:TimesNewRoman;
	mso-font-alt:"Times New Roman";
	mso-font-charset:0;
	mso-generic-font-family:auto;
	mso-font-pitch:auto;
	mso-font-signature:0 0 0 0 0 0;}
@font-face
	{font-family:TimesNewRomanPSMT;
	mso-font-alt:"Times New Roman";
	mso-font-charset:0;
	mso-generic-font-family:auto;
	mso-font-pitch:auto;
	mso-font-signature:0 0 0 0 0 0;}
 /* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{mso-style-parent:"";
	margin:0mm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
h1
	{mso-style-next:Normal;
	margin-top:12.0pt;
	margin-right:0mm;
	margin-bottom:3.0pt;
	margin-left:0mm;
	mso-pagination:widow-orphan;
	mso-outline-level:1;
	font-size:16.0pt;
	font-family:Arial;
	mso-font-kerning:16.0pt;}
h2
	{mso-style-next:Normal;
	margin-top:12.0pt;
	margin-right:0mm;
	margin-bottom:3.0pt;
	margin-left:0mm;
	mso-pagination:widow-orphan;
	mso-outline-level:2;
	font-size:14.0pt;
	font-family:Arial;
	font-style:italic;}
h3
	{margin-right:0mm;
	mso-margin-top-alt:auto;
	mso-margin-bottom-alt:auto;
	margin-left:0mm;
	mso-pagination:widow-orphan;
	mso-outline-level:3;
	font-size:13.5pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Arial Unicode MS";}
h4
	{mso-style-next:Normal;
	margin:0mm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	mso-outline-level:4;
	tab-stops:108.0pt;
	font-size:10.0pt;
	mso-bidi-font-size:12.0pt;
	font-family:Arial;
	mso-ansi-language:EN-GB;}
h5
	{mso-style-next:Normal;
	margin:0mm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	mso-outline-level:5;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-ansi-language:EN-GB;}
p.MsoCommentText, li.MsoCommentText, div.MsoCommentText
	{margin:0mm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:10.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
span.MsoCommentReference
	{mso-ansi-font-size:8.0pt;
	mso-bidi-font-size:8.0pt;}
p.MsoListBullet, li.MsoListBullet, div.MsoListBullet
	{mso-style-update:auto;
	margin-top:0mm;
	margin-right:0mm;
	margin-bottom:0mm;
	margin-left:28.5pt;
	margin-bottom:.0001pt;
	text-indent:-18.0pt;
	mso-pagination:widow-orphan;
	mso-list:l0 level1 lfo3;
	tab-stops:list 28.5pt 36.0pt;
	font-size:10.0pt;
	mso-bidi-font-size:12.0pt;
	font-family:Arial;
	mso-fareast-font-family:"Times New Roman";
	mso-ansi-language:EN-GB;}
p.MsoListNumber, li.MsoListNumber, div.MsoListNumber
	{margin-top:0mm;
	margin-right:0mm;
	margin-bottom:0mm;
	margin-left:18.0pt;
	margin-bottom:.0001pt;
	text-indent:-18.0pt;
	mso-pagination:widow-orphan;
	mso-list:l1 level1 lfo6;
	tab-stops:list 18.0pt 36.0pt;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
p.MsoBodyText, li.MsoBodyText, div.MsoBodyText
	{margin-top:0mm;
	margin-right:0mm;
	margin-bottom:6.0pt;
	margin-left:0mm;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
p.MsoBodyTextFirstIndent, li.MsoBodyTextFirstIndent, div.MsoBodyTextFirstIndent
	{mso-style-parent:"Corps de texte";
	margin-top:0mm;
	margin-right:0mm;
	margin-bottom:6.0pt;
	margin-left:0mm;
	text-indent:10.5pt;
	mso-pagination:widow-orphan;
	font-size:10.0pt;
	mso-bidi-font-size:12.0pt;
	font-family:Arial;
	mso-fareast-font-family:"Times New Roman";
	mso-ansi-language:EN-GB;}
a:link, span.MsoHyperlink
	{color:blue;
	text-decoration:underline;
	text-underline:single;}
a:visited, span.MsoHyperlinkFollowed
	{color:purple;
	text-decoration:underline;
	text-underline:single;}
p.msocommentsubject, li.msocommentsubject, div.msocommentsubject
	{mso-style-name:msocommentsubject;
	mso-style-parent:Commentaire;
	mso-style-next:Commentaire;
	margin:0mm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:10.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";
	font-weight:bold;}
p.Paper, li.Paper, div.Paper
	{mso-style-name:Paper;
	mso-style-parent:"Liste à numéros";
	margin-top:0mm;
	margin-right:0mm;
	margin-bottom:0mm;
	margin-left:18.0pt;
	margin-bottom:.0001pt;
	text-indent:-18.0pt;
	mso-pagination:widow-orphan;
	mso-list:l1 level1 lfo6;
	tab-stops:list 18.0pt 36.0pt;
	font-size:10.0pt;
	mso-bidi-font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";
	mso-ansi-language:EN-GB;
	font-style:italic;}
p.Default, li.Default, div.Default
	{mso-style-name:Default;
	mso-style-parent:"";
	margin:0mm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	mso-layout-grid-align:none;
	font-size:10.0pt;
	font-family:TimesNewRoman;
	mso-fareast-font-family:"Times New Roman";
	mso-bidi-font-family:"Times New Roman";}
p.a, li.a, div.a
	{mso-style-name:"\.\.";
	mso-style-parent:Default;
	mso-style-next:Default;
	margin-top:0mm;
	margin-right:0mm;
	margin-bottom:4.0pt;
	margin-left:0mm;
	mso-pagination:widow-orphan;
	mso-layout-grid-align:none;
	font-size:12.0pt;
	font-family:TimesNewRoman;
	mso-fareast-font-family:"Times New Roman";
	mso-bidi-font-family:"Times New Roman";}
span.CommentaireCar
	{mso-style-name:"Commentaire Car";
	mso-ansi-language:FR;
	mso-fareast-language:FR;
	mso-bidi-language:AR-SA;}
@page Section1
	{size:595.3pt 841.9pt;
	margin:70.9pt 70.9pt 70.9pt 70.9pt;
	mso-header-margin:35.45pt;
	mso-footer-margin:35.45pt;
	mso-paper-source:0;}
div.Section1
	{page:Section1;}
 /* List Definitions */
@list l0
	{mso-list-id:92828331;
	mso-list-template-ids:-762441242;}
@list l1
	{mso-list-id:1440561656;
	mso-list-template-ids:1660581158;}
ol
	{margin-bottom:0mm;}
ul
	{margin-bottom:0mm;}
-->
</style>
<!--[if gte mso 9]><xml>
 <o:shapedefaults v:ext="edit" spidmax="1034">
  <o:colormru v:ext="edit" colors="#ffc"/>
  <o:colormenu v:ext="edit" fillcolor="#ffc"/>
 </o:shapedefaults></xml><![endif]--><!--[if gte mso 9]><xml>
 <o:shapelayout v:ext="edit">
  <o:idmap v:ext="edit" data="1"/>
 </o:shapelayout></xml><![endif]--><o:SmartTagType name="State" namespaceuri="urn:schemas-microsoft-com:office:smarttags"></o:SmartTagType><o:SmartTagType name="City" namespaceuri="urn:schemas-microsoft-com:office:smarttags"></o:SmartTagType><o:SmartTagType name="place" namespaceuri="urn:schemas-microsoft-com:office:smarttags"></o:SmartTagType><o:SmartTagType name="PlaceType" namespaceuri="urn:schemas-microsoft-com:office:smarttags"></o:SmartTagType><o:SmartTagType name="PlaceName" namespaceuri="urn:schemas-microsoft-com:office:smarttags"></o:SmartTagType><o:SmartTagType name="date" namespaceuri="urn:schemas-microsoft-com:office:smarttags"></o:SmartTagType>
</head>

<body bgcolor="#ffffcc" lang=FR link=blue vlink=purple style='tab-interval:
35.45pt'>

<div class=Section1>

<div align=center>

<table border=0 cellspacing=0 cellpadding=0 width="90%" style='width:90.06%;
 margin-left:-.4pt;border-collapse:collapse;mso-padding-alt:0mm 3.5pt 0mm 3.5pt'>
 <tr style='mso-yfti-irow: 0'>
  <td width=203 valign=top style='width:151.9pt;border-top:none;border-left:
  none;border-bottom:solid windowtext .5pt;border-right:solid windowtext .5pt;
  background:#FFFFB3;padding:0mm 3.5pt 0mm 3.5pt'>
  <p class=MsoNormal><!--[if gte vml 1]><v:shapetype id="_x0000_t75"
   coordsize="21600,21600" o:spt="75" o:preferrelative="t" path="m@4@5l@4@11@9@11@9@5xe"
   filled="f" stroked="f">
   <v:stroke joinstyle="miter"/>
   <v:formulas>
    <v:f eqn="if lineDrawn pixelLineWidth 0"/>
    <v:f eqn="sum @0 1 0"/>
    <v:f eqn="sum 0 0 @1"/>
    <v:f eqn="prod @2 1 2"/>
    <v:f eqn="prod @3 21600 pixelWidth"/>
    <v:f eqn="prod @3 21600 pixelHeight"/>
    <v:f eqn="sum @0 0 1"/>
    <v:f eqn="prod @6 1 2"/>
    <v:f eqn="prod @7 21600 pixelWidth"/>
    <v:f eqn="sum @8 21600 0"/>
    <v:f eqn="prod @7 21600 pixelHeight"/>
    <v:f eqn="sum @10 21600 0"/>
   </v:formulas>
   <v:path o:extrusionok="f" gradientshapeok="t" o:connecttype="rect"/>
   <o:lock v:ext="edit" aspectratio="t"/>
  </v:shapetype><v:shape id="_x0000_i1025" type="#_x0000_t75" alt="Nightingale"
   style='width:144.75pt;height:53.25pt'>
   <v:imagedata src="./papers_fichiers/image001.jpg" o:href="http://ismir2002.ismir.net/index_fichiers/ngale1.jpg"/>
  </v:shape><![endif]--><![if !vml]><img width=193 height=71
  src="papers_fichiers/image002.jpg" alt=Nightingale v:shapes="_x0000_i1025"><![endif]><span
  lang=EN-GB style='mso-ansi-language:EN-GB'><o:p></o:p></span></p>
  <p class=MsoNormal style='margin-top:30.0pt;tab-stops:14.2pt 108.0pt'><span
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial'><a
  href="index-2.html">General information</a><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt 108.0pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><a href="calendar.html"><span lang=FR
  style='mso-ansi-language:FR'>Important dates</span></a></span><span
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial'><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt 108.0pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><a href="submissions.html"><span lang=FR
  style='mso-ansi-language:FR'>Submission information</span></a></span><span
  lang=EN-GB style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:
  Arial'> </span><span style='font-size:10.0pt;mso-bidi-font-size:12.0pt;
  font-family:Arial'><o:p></o:p></span></p>
  <h4 style='tab-stops:14.2pt 108.0pt'><span lang=EN-GB style='font-weight:
  normal'><a href="program.html">Conference program</a><o:p></o:p></span></h4>
  <h5 style='tab-stops:14.2pt'><span lang=EN-GB style='font-size:10.0pt;
  mso-bidi-font-size:12.0pt;font-family:Arial;font-weight:normal'><span
  style='mso-tab-count:1'>     </span><a href="full-program.html">Detailed
  program</a><o:p></o:p></span></h5>
  <h5 style='tab-stops:14.2pt'><span lang=EN-GB style='font-size:10.0pt;
  mso-bidi-font-size:12.0pt;font-family:Arial;font-weight:normal'><span
  style='mso-tab-count:1'>     </span><a href="speakers.html">Keynote speakers</a><o:p></o:p></span></h5>
  <p class=MsoNormal style='tab-stops:14.2pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><span style='mso-tab-count:1'>     </span><a
  href="metadata.html">Special Metadata Session</a><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><span style='mso-tab-count:1'>     </span><b>Accepted
  papers<o:p></o:p></b></span></p>
  <p class=MsoNormal style='margin-left:28.4pt;text-indent:-14.2pt;tab-stops:
  14.2pt'><span lang=EN-GB style='font-size:10.0pt;mso-bidi-font-size:12.0pt;
  font-family:Arial;mso-ansi-language:EN-GB'><a href="posters.html">Accepted
  posters/short papers</a><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><span style='mso-tab-count:1'>     </span></span><span
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial'><a
  href="tutorials.html"><span lang=EN-GB style='mso-ansi-language:EN-GB'>Tutorials</span></a></span><span
  lang=EN-GB style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:
  Arial;mso-ansi-language:EN-GB'><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><span style='mso-tab-count:1'>     </span></span><span
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial'><a
  href="panels.html"><span lang=EN-GB style='mso-ansi-language:EN-GB'>Panels</span></a></span><span
  lang=EN-GB style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:
  Arial;mso-ansi-language:EN-GB'><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><span style='mso-tab-count:1'>     </span><a
  href="visit.html">Professional visit</a><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><span style='mso-tab-count:1'>     </span><a
  href="exhibits.html">Exhibits</a><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><span style='mso-tab-count:1'>     </span><a
  href="social.html">Social events</a><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt 108.0pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><a
  href="https://www.ircam.fr/ismir2002-register.html">Registration</a><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt 108.0pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><a href="venue.html">Conference venue</a><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt 108.0pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><a href="committee.html">Conference committee</a><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:108.0pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><a href="photos.html">Photos from the conference</a><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:108.0pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><a href="order-form.pdf">Conference proceedings</a><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt 108.0pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><a href="mailing-list.html">Mailing-list</a><o:p></o:p></span></p>
  <h4 style='margin-top:12.0pt'><span lang=EN-GB>Hosted by</span></h4>
  <p class=MsoNormal><span lang=EN-GB style='font-size:10.0pt;mso-bidi-font-size:
  12.0pt;font-family:Arial;mso-ansi-language:EN-GB'>&nbsp;<o:p></o:p></span></p>
  <p class=MsoNormal align=center style='text-align:center'><span
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial'><a
  href="http://www.ircam.fr/"><span style='text-decoration:none;text-underline:
  none'><!--[if gte vml 1]><v:shape id="_x0000_i1026" type="#_x0000_t75"
   href="http://www.ircam.fr/" style='width:87pt;height:51.75pt'
   o:allowoverlap="f" o:button="t">
   <v:fill o:detectmouseclick="t"/>
   <v:imagedata src="./papers_fichiers/image003.wmz" o:althref="./papers_fichiers/image004.pcz"
    o:title=""/>
  </v:shape><![endif]--><![if !vml]><img border=0 width=116 height=69
  src="papers_fichiers/image005.gif" v:shapes="_x0000_i1026"><![endif]></span></a></span><span
  lang=EN-GB style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:
  Arial;mso-ansi-language:EN-GB'><o:p></o:p></span></p>
  <h4 style='margin-top:12.0pt'><span lang=EN-GB>Sponsors and partners</span></h4>
  <p class=MsoNormal align=center style='text-align:center'><br>
  <a href="http://www.mairie-paris.fr/"><span style='color:windowtext;
  text-decoration:none;text-underline:none'><!--[if gte vml 1]><v:shape id="_x0000_i1027"
   type="#_x0000_t75" alt="Mairie de Paris" style='width:123.75pt;height:14.25pt'>
   <v:imagedata src="./papers_fichiers/image006.gif" o:href="http://www.mairie-paris.fr/images/commun/logo_mairie_paris.gif"/>
  </v:shape><![endif]--><![if !vml]><img border=0 width=165 height=19
  src="papers_fichiers/image006.gif" alt="Mairie de Paris" v:shapes="_x0000_i1027"><![endif]></span></a><br>
  <br>
  <a href="http://www.nsf.gov/"><span style='color:windowtext;text-decoration:
  none;text-underline:none'><!--[if gte vml 1]><v:shape id="_x0000_i1028"
   type="#_x0000_t75" alt="" style='width:68.25pt;height:75pt'>
   <v:imagedata src="./papers_fichiers/image007.gif" o:href="http://ismir2001.indiana.edu/images/nsflogo1.gif"/>
  </v:shape><![endif]--><![if !vml]><img border=0 width=91 height=100
  src="papers_fichiers/image007.gif" v:shapes="_x0000_i1028"><![endif]></span></a><br>
  <br>
  <a href="http://www.indiana.edu/"><span style='color:windowtext;text-decoration:
  none;text-underline:none'><!--[if gte vml 1]><v:shape id="_x0000_i1029"
   type="#_x0000_t75" alt="IU seal, red on white, small" style='width:68.25pt;
   height:68.25pt' o:ole="">
   <v:imagedata src="./papers_fichiers/image008.wmz" o:title=""/>
  </v:shape><![endif]--><![if !vml]><img border=0 width=91 height=91
  src="papers_fichiers/image009.gif" alt="IU seal, red on white, small"
  v:shapes="_x0000_i1029"><![endif]><!--[if gte mso 9]><xml>
   <o:OLEObject Type="Embed" ProgID="Word.Picture.8" ShapeID="_x0000_i1029"
    DrawAspect="Content" ObjectID="_1103543604">
   </o:OLEObject>
  </xml><![endif]--></span></a><br>
  <br>
  <a href="http://www.cnrs.fr/"><span style='text-decoration:none;text-underline:
  none'><!--[if gte vml 1]><v:shape id="_x0000_i1030" type="#_x0000_t75"
   style='width:98.25pt;height:42pt'>
   <v:imagedata src="./papers_fichiers/image010.gif" o:title="art93-4[1]"/>
  </v:shape><![endif]--><![if !vml]><img border=0 width=131 height=56
  src="papers_fichiers/image011.gif" v:shapes="_x0000_i1030"><![endif]></span></a><span
  lang=EN-GB style='mso-ansi-language:EN-GB'><o:p></o:p></span></p>
  <p class=MsoNormal><span lang=EN-GB style='mso-ansi-language:EN-GB'>&nbsp;<o:p></o:p></span></p>
  </td>
  <td width=537 valign=top style='width:402.95pt;border:none;border-bottom:
  solid windowtext .5pt;mso-border-left-alt:solid windowtext .5pt;background:
  #FFFFCC;padding:0mm 3.5pt 0mm 3.5pt'>
  <h1 align=center style='margin-top:0mm;text-align:center'><span lang=EN-GB
  style='font-size:14.0pt;mso-bidi-font-size:16.0pt;mso-ansi-language:EN-GB'>ISMIR
  2002<br>
  3rd International Conference on<br>
  Music Information Retrieval<o:p></o:p></span></h1>
  <h2 align=center style='text-align:center'><span lang=EN-GB style='font-size:
  12.0pt;mso-bidi-font-size:14.0pt;mso-ansi-language:EN-GB'>IRCAM – Centre
  Pompidou<br>
  Paris, France<br>
  <st1:date Month="10" Day="13" Year="2002">October 13-17, 2002<o:p></o:p></span></h2>
  <div style='border:solid windowtext .5pt;padding:1.0pt 4.0pt 1.0pt 4.0pt'>
  <p class=MsoBodyTextFirstIndent style='margin-top:0mm;margin-right:14.2pt;
  margin-bottom:6.0pt;margin-left:14.2pt;text-align:justify;text-indent:0mm;
  border:none;mso-border-alt:solid windowtext .5pt;padding:0mm;mso-padding-alt:
  1.0pt 4.0pt 1.0pt 4.0pt'><span lang=EN-GB></st1:date>This page now includes a
  link (through the <i>&#9658;</i>icon) to the full text of the final version
  of the conference papers. These texts are © IRCAM – Centre Pompidou in this
  form. Authors have retained the rights to their original texts.</span></p>
  <p class=MsoBodyTextFirstIndent style='margin-top:0mm;margin-right:14.2pt;
  margin-bottom:0mm;margin-left:14.2pt;margin-bottom:.0001pt;text-align:justify;
  text-indent:0mm;border:none;mso-border-alt:solid windowtext .5pt;padding:
  0mm;mso-padding-alt:1.0pt 4.0pt 1.0pt 4.0pt'><span lang=EN-GB>The printed
  version of the proceedings, which also contains introductions, the abstracts,
  a table of contents and the author index, can be ordered <a
  href="order-form.pdf">here</a>.</span></p>
  </div>
  <h3><span lang=EN-US style='mso-ansi-language:EN-US'>ACCEPTED PAPERS<o:p></o:p></span></h3>
  <p class=MsoBodyTextFirstIndent style='text-align:justify'><span lang=EN-GB>The
  selected submissions in this category will be presented by their authors the
  main conference room in 30 minute long talks (with computer and audio
  output).</span></p>
  <h3><span lang=EN-GB style='mso-ansi-language:EN-GB'>Similarity and
  Recognition<o:p></o:p></span></h3>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>1.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Yongmoo E. Kim (MIT Media Lab) and Brian Whitman:<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Singer Identification in
  Popular Music Recordings Using Voice Coding <a style='mso-comment-reference:
  Abstract_1'>Features</a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_1"
  onmouseover="msoCommentShow('_anchor_1','_com_1')"
  onmouseout="msoCommentHide('_com_1')" href="#_msocom_1" language=JavaScript
  name="_msoanchor_1">[Abstract1]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP05-3.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>2.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Daniel P.W. Ellis (<st1:place><st1:PlaceName>Columbia</st1:PlaceName>
  <st1:PlaceType>University</st1:PlaceType></st1:place>), Brian Whitman (MIT
  Media Lab), Adam Berenzweig (<st1:place><st1:PlaceName>Columbia</st1:PlaceName>
  <st1:PlaceType>University</st1:PlaceType></st1:place>) and Steve Lawrence
  (NEC Research Institute):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>The Quest for Ground
  Truth in </span></b><b><span lang=EN-GB style='mso-bidi-font-size:10.0pt;
  font-style:normal'>Musical Artist <a style='mso-comment-reference:Abstract_2'>Similarity</a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_2"
  onmouseover="msoCommentShow('_anchor_2','_com_2')"
  onmouseout="msoCommentHide('_com_2')" href="#_msocom_2" language=JavaScript
  name="_msoanchor_2">[Abstract2]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span><b><span
  lang=EN-GB style='mso-bidi-font-size:10.0pt;font-style:normal'> </span></b><span
  lang=EN-GB style='font-style:normal'><a href="proceedings/02-FP05-4.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>3.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Jouni Paulus and Anssi Klapuri (<st1:place><st1:PlaceName>Tampere</st1:PlaceName>
  <st1:PlaceType>University</st1:PlaceType></st1:place> of Technology):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Measuring the Similarity
  of Rhythmic <a style='mso-comment-reference:Abstract_3'>Patterns</a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_3"
  onmouseover="msoCommentShow('_anchor_3','_com_3')"
  onmouseout="msoCommentHide('_com_3')" href="#_msocom_3" language=JavaScript
  name="_msoanchor_3">[Abstract3]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP05-1.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>4.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Jean-Julien Aucouturier and François Pachet (Sony Computer Science
  Lab., Paris):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Music Similarity
  Measures&nbsp;: What’s the <a style='mso-comment-reference:Abstract_4'>use?&nbsp;</a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_4"
  onmouseover="msoCommentShow('_anchor_4','_com_4')"
  onmouseout="msoCommentHide('_com_4')" href="#_msocom_4" language=JavaScript
  name="_msoanchor_4">[Abstract4]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP05-2.pdf">&#9658;</a></span></p>
  <h3><span lang=EN-GB style='mso-ansi-language:EN-GB'>Summarization<o:p></o:p></span></h3>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>5.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Keiji Hirata (NTT Communications Science Laboratories) and Shu
  Matsuda (Digital Art Creation):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Interactive Music Summarization
  based on <a style='mso-comment-reference:Abstract_5'>GTTM</a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_5"
  onmouseover="msoCommentShow('_anchor_5','_com_5')"
  onmouseout="msoCommentHide('_com_5')" href="#_msocom_5" language=JavaScript
  name="_msoanchor_5">[Abstract5]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP03-2.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB style='mso-bidi-font-size:
  10.0pt'>6.<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  </span></span><![endif]><span lang=EN-GB>Geoffroy Peeters, Amaury La Burthe
  and Xavier Rodet (IRCAM):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Toward Automatic Music
  Audio Summary Generation from </span></b><b><span lang=EN-GB
  style='mso-bidi-font-size:10.0pt;font-style:normal'>Signal <a
  style='mso-comment-reference:Abstract_6;mso-comment-date: 20020702T2318'>Analysis</a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_6"
  onmouseover="msoCommentShow('_anchor_6','_com_6')"
  onmouseout="msoCommentHide('_com_6')" href="#_msocom_6" language=JavaScript
  name="_msoanchor_6">[Abstract6]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span><b><span
  lang=EN-GB style='mso-bidi-font-size:10.0pt;font-style:normal'> </span></b><span
  lang=EN-GB style='font-style:normal'><a href="proceedings/02-FP03-3.pdf">&#9658;</a></span><span
  lang=EN-GB style='mso-bidi-font-size:10.0pt'><o:p></o:p></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>7.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Matthew Cooper and Jonathan Foote (FX <st1:City><st1:place>Palo
  Alto</st1:place></st1:City> Laboratory):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Automatic Music
  Summarization via Similarity <a style='mso-comment-reference:Abstract_7;
  mso-comment-date: 20020702T1920'>Analysis</a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_7"
  onmouseover="msoCommentShow('_anchor_7','_com_7')"
  onmouseout="msoCommentHide('_com_7')" href="#_msocom_7" language=JavaScript
  name="_msoanchor_7">[Abstract7]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP03-1.pdf">&#9658;</a></span></p>
  <h3><span lang=EN-GB style='mso-ansi-language:EN-GB'>Indexation,
  classification, analysis<o:p></o:p></span></h3>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>8.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Roger B. Dannenberg and Ning Hu (<st1:place><st1:PlaceName>Carnegie</st1:PlaceName>
  <st1:PlaceName>Mellon</st1:PlaceName> <st1:PlaceType>University</st1:PlaceType></st1:place>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Pattern Discovery
  Techniques for Music <a style='mso-comment-reference:Abstract_8'>Audio</a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_8"
  onmouseover="msoCommentShow('_anchor_8','_com_8')"
  onmouseout="msoCommentHide('_com_8')" href="#_msocom_8" language=JavaScript
  name="_msoanchor_8">[Abstract8]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP02-3.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>9.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Cheng Yang (<st1:place><st1:PlaceName>Stanford</st1:PlaceName> <st1:PlaceType>University</st1:PlaceType></st1:place>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>MACSIS: A Scalable
  Acoustic Index for Content-Based Music <a style='mso-comment-reference:Abstract_9;
  mso-comment-date: 20020702T2318'>Retrieval</a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_9"
  onmouseover="msoCommentShow('_anchor_9','_com_9')"
  onmouseout="msoCommentHide('_com_9')" href="#_msocom_9" language=JavaScript
  name="_msoanchor_9">[Abstract9]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP02-2.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>10.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Andreas Rauber (<st1:place><st1:PlaceName>Vienna</st1:PlaceName> <st1:PlaceType>University</st1:PlaceType></st1:place>
  of Technology), Elias Pampalk (Austrian Research Institute for Artificial
  Intelligence) and Dieter Merkl (<st1:place><st1:PlaceName>Vienna</st1:PlaceName>
  <st1:PlaceType>University</st1:PlaceType></st1:place> of Technology):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Using Psycho-Acoustic
  Models and Self-Organizing Maps to Create a Hierarchical Structuring of Music
  by Musical <a style='mso-comment-reference:Abstract_10;mso-comment-date: 20020702T2318'>Styles</a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_10"
  onmouseover="msoCommentShow('_anchor_10','_com_10')"
  onmouseout="msoCommentHide('_com_10')" href="#_msocom_10"
  language=JavaScript name="_msoanchor_10">[Abstract10]</a><![endif]><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt;display:
  none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP02-4.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB style='mso-bidi-font-size:
  10.0pt'>11.<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Brian Whitman and <st1:City><st1:place>Paris</st1:place></st1:City>
  Smaragdis (MIT Media Lab):<br>
  </span><b><span lang=EN-GB style='mso-bidi-font-size:10.0pt;font-style:normal'>Combining
  Musical and Cultural Features for Intelligent Style Detection </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt'><a
  style='mso-comment-reference:Abstract_11'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_11"
  onmouseover="msoCommentShow('_anchor_11','_com_11')"
  onmouseout="msoCommentHide('_com_11')" href="#_msocom_11"
  language=JavaScript name="_msoanchor_11">[Abstract11]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='mso-bidi-font-size:10.0pt;font-style:normal'> </span></b><span
  lang=EN-GB style='font-style:normal'><a href="proceedings/02-FP02-1.pdf">&#9658;</a></span><span
  lang=EN-GB style='mso-bidi-font-size:10.0pt'><o:p></o:p></span></p>
  <h3><span lang=EN-GB style='mso-ansi-language:EN-GB'>Usability<o:p></o:p></span></h3>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>12.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Alexandra Uitdenbogerd and Ron van Schyndel (<st1:place><st1:PlaceName>RMIT</st1:PlaceName>
  <st1:PlaceType>University</st1:PlaceType></st1:place>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>A Review of Factors
  Affecting Music Recommender Success </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt'><a
  style='mso-comment-reference:Abstract_12'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_12"
  onmouseover="msoCommentShow('_anchor_12','_com_12')"
  onmouseout="msoCommentHide('_com_12')" href="#_msocom_12"
  language=JavaScript name="_msoanchor_12">[Abstract12]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP07-1.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>13.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Steffen Pauws (Philips Research <st1:City><st1:place>Eindhoven</st1:place></st1:City>)
  and <st1:State><st1:place>Berry</st1:place></st1:State> Eggen (Philips
  Research <st1:City><st1:place>Eindhoven</st1:place></st1:City> and Technische
  Universiteit <st1:City><st1:place>Eindhoven</st1:place></st1:City>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>PATS: Realization and
  user evaluation of an automatic playlist generator </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt'><a
  style='mso-comment-reference:Abstract_13'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_13"
  onmouseover="msoCommentShow('_anchor_13','_com_13')"
  onmouseout="msoCommentHide('_com_13')" href="#_msocom_13"
  language=JavaScript name="_msoanchor_13">[Abstract13]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP07-4.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>14.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Joe Futrelle and J. Stephen Downie (<st1:place><st1:PlaceType>University</st1:PlaceType>
  of <st1:PlaceName>Illinois</st1:PlaceName></st1:place>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Interdisciplinary Communities
  and Research Issues in Music Information Retrieval </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt'><a
  style='mso-comment-reference:Abstract_14'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_14"
  onmouseover="msoCommentShow('_anchor_14','_com_14')"
  onmouseout="msoCommentHide('_com_14')" href="#_msocom_14"
  language=JavaScript name="_msoanchor_14">[Abstract14]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP07-3.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>15.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Ann Blandford and Hanna Stelmaszewska (<st1:place><st1:PlaceType>University</st1:PlaceType>
  <st1:PlaceType>College</st1:PlaceType></st1:place> <st1:City><st1:place>London</st1:place></st1:City>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Usability of Musical
  Digital Libraries: a Multimodal Analysis </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt'><a
  style='mso-comment-reference:Abstract_15'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_15"
  onmouseover="msoCommentShow('_anchor_15','_com_15')"
  onmouseout="msoCommentHide('_com_15')" href="#_msocom_15"
  language=JavaScript name="_msoanchor_15">[Abstract15]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP07-5.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>16.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Ja-Young Kim and Nicholas J. Belkin (<st1:place><st1:PlaceName>Rutgers</st1:PlaceName>
  <st1:PlaceType>University</st1:PlaceType></st1:place>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Categories of Music
  Description and Search Terms and Phrases Used by Non-Music Experts </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt'><a
  style='mso-comment-reference:Abstract_16'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_16"
  onmouseover="msoCommentShow('_anchor_16','_com_16')"
  onmouseout="msoCommentHide('_com_16')" href="#_msocom_16"
  language=JavaScript name="_msoanchor_16">[Abstract16]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP07-2.pdf">&#9658;</a></span></p>
  <h3><span lang=EN-GB style='mso-ansi-language:EN-GB'>Query By Example<o:p></o:p></span></h3>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>17.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Jaap Haitsma and Ton Kalker (Philips Research <st1:City><st1:place>Eindhoven</st1:place></st1:City>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>A Highly Robust Audio
  Fingerprinting System </span></b><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt'><a style='mso-comment-reference:Abstract_17'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_17"
  onmouseover="msoCommentShow('_anchor_17','_com_17')"
  onmouseout="msoCommentHide('_com_17')" href="#_msocom_17"
  language=JavaScript name="_msoanchor_17">[Abstract17]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP04-2.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>18.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB style='font-size:7.0pt'><span style="mso-spacerun: yes"> </span></span><span
  lang=EN-GB>Jeremy Pickens (University of Massachusetts Amherst), Juan Pablo
  Bello (University of London), Tim Crawford (King’s College), Matthew Dovey
  (Oxford University), Giuliano Monti (University of London) and Mark Sandler
  (University of London):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Polyphonic Score
  Retrieval Using Polyphonic Audio Queries: A Harmonic Modeling Approach </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt'><a
  style='mso-comment-reference:Abstract_18'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_18"
  onmouseover="msoCommentShow('_anchor_18','_com_18')"
  onmouseout="msoCommentHide('_com_18')" href="#_msocom_18"
  language=JavaScript name="_msoanchor_18">[Abstract18]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP04-6.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>19.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB style='font-size:7.0pt'><span style="mso-spacerun: yes"> </span></span><span
  lang=EN-GB>Jungmin Song, So-Young Bae and Kyoungro Yoon (LG Electronics):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Mid-Level Music Melody
  Representation of Polyphonic Audio for Query-by-Humming System </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt'><a
  style='mso-comment-reference:Abstract_19'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_19"
  onmouseover="msoCommentShow('_anchor_19','_com_19')"
  onmouseout="msoCommentHide('_com_19')" href="#_msocom_19"
  language=JavaScript name="_msoanchor_19">[Abstract19]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP04-5.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>20.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB style='font-size:7.0pt'><span style="mso-spacerun: yes"> </span></span><span
  lang=EN-GB>Shyamala Doraisamy and Stefan M. Rüger (<st1:place><st1:PlaceName>Imperial</st1:PlaceName>
  <st1:PlaceType>College</st1:PlaceType></st1:place> <st1:City><st1:place>London</st1:place></st1:City>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>A Comparative and
  Fault-tolerance Study of the Use of N-grams with Polyphonic Music </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt'><a
  style='mso-comment-reference:Abstract_20'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_20"
  onmouseover="msoCommentShow('_anchor_20','_com_20')"
  onmouseout="msoCommentHide('_com_20')" href="#_msocom_20"
  language=JavaScript name="_msoanchor_20">[Abstract20]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP04-1.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>21.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB style='font-size:7.0pt'><span style="mso-spacerun: yes"> </span></span><span
  lang=EN-GB>Colin Meek and William Birmingham (<st1:place><st1:PlaceType>University</st1:PlaceType>
  of <st1:PlaceName>Michigan</st1:PlaceName></st1:place>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Johnny Can’t Sing: A
  Comprehensive Error Model for Sung Music Queries </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt'><a
  style='mso-comment-reference:Abstract_21'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_21"
  onmouseover="msoCommentShow('_anchor_21','_com_21')"
  onmouseout="msoCommentHide('_com_21')" href="#_msocom_21"
  language=JavaScript name="_msoanchor_21">[Abstract21]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP04-4.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>22.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>L. P. Clarisse, J. P. Martens, M. Lesaffre, B. De Baets, H. De
  Meyer and M. Leman (<st1:place><st1:PlaceName>Ghent</st1:PlaceName> <st1:PlaceType>University</st1:PlaceType></st1:place>)&nbsp;:<br>
  </span><b><span lang=EN-GB style='font-style:normal'>An Auditory Model Based
  Transcriber of Singing Sequences </span></b><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt'><a style='mso-comment-reference:Abstract_22'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_22"
  onmouseover="msoCommentShow('_anchor_22','_com_22')"
  onmouseout="msoCommentHide('_com_22')" href="#_msocom_22"
  language=JavaScript name="_msoanchor_22">[Abstract22]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP04-3.pdf">&#9658;</a></span></p>
  <h3><span lang=EN-GB style='mso-ansi-language:EN-GB'>Preprocessing: encoding,
  segmentation…<o:p></o:p></span></h3>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>23.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Christopher Raphael (<st1:place><st1:PlaceType>University</st1:PlaceType>
  of <st1:PlaceName>Massachusetts</st1:PlaceName></st1:place> <st1:City><st1:place>Amherst</st1:place></st1:City>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Automatic Transcription
  of Piano Music </span></b><span class=MsoCommentReference><span lang=EN-GB
  style='font-size:8.0pt'><a style='mso-comment-reference:Abstract_23'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_23"
  onmouseover="msoCommentShow('_anchor_23','_com_23')"
  onmouseout="msoCommentHide('_com_23')" href="#_msocom_23"
  language=JavaScript name="_msoanchor_23">[Abstract23]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP01-2.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>24.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB style='font-size:7.0pt'><span style="mso-spacerun: yes"> </span></span><span
  lang=EN-GB>Jürgen Kilian (<st1:place><st1:PlaceName>Darmstadt</st1:PlaceName>
  <st1:PlaceType>University</st1:PlaceType></st1:place> of Technology) and
  Holger H. Hoos (<st1:place><st1:PlaceType>University</st1:PlaceType> of <st1:PlaceName>British
  Columbia</st1:PlaceName></st1:place>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Voice Separation – A
  Local Optimization Approach </span></b><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt'><a style='mso-comment-reference:Abstract_24'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_24"
  onmouseover="msoCommentShow('_anchor_24','_com_24')"
  onmouseout="msoCommentHide('_com_24')" href="#_msocom_24"
  language=JavaScript name="_msoanchor_24">[Abstract24]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP01-6.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>25.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Anna Pienimäki (<st1:place><st1:PlaceType>University</st1:PlaceType>
  of <st1:PlaceName>Helsinki</st1:PlaceName></st1:place>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Indexing Music Databases
  Using Automatic Extraction of Frequent Phrases </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt'><a
  style='mso-comment-reference:Abstract_25'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_25"
  onmouseover="msoCommentShow('_anchor_25','_com_25')"
  onmouseout="msoCommentHide('_com_25')" href="#_msocom_25"
  language=JavaScript name="_msoanchor_25">[Abstract25]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP01-4.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>26.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB style='font-size:7.0pt'><span style="mso-spacerun: yes"> </span></span><span
  lang=EN-GB>George Tzanetakis, Andrey Ermolinskiy and Perry Cook (<st1:place><st1:PlaceName>Princeton</st1:PlaceName>
  <st1:PlaceType>University</st1:PlaceType></st1:place>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Pitch Histograms in
  Audio and Symbolic Music Information Retrieval </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt'><a
  style='mso-comment-reference:Abstract_26'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_26"
  onmouseover="msoCommentShow('_anchor_26','_com_26')"
  onmouseout="msoCommentHide('_com_26')" href="#_msocom_26"
  language=JavaScript name="_msoanchor_26">[Abstract26]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP01-5.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>27.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Massimo Melucci and Nicola Orio (<st1:place><st1:PlaceType>University</st1:PlaceType>
  of <st1:PlaceName>Padova</st1:PlaceName></st1:place>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>A Comparison of Manual
  and Automatic Melody Segmentation </span></b><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt'><a style='mso-comment-reference:Abstract_27'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_27"
  onmouseover="msoCommentShow('_anchor_27','_com_27')"
  onmouseout="msoCommentHide('_com_27')" href="#_msocom_27"
  language=JavaScript name="_msoanchor_27">[Abstract27]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP01-1.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>28.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Hui Jin and H. V. Jagadish (University of Michigan):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Indexing Hidden Markov
  Models for Music Retrieval </span></b><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt'><a style='mso-comment-reference:Abstract_28'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_28"
  onmouseover="msoCommentShow('_anchor_28','_com_28')"
  onmouseout="msoCommentHide('_com_28')" href="#_msocom_28"
  language=JavaScript name="_msoanchor_28">[Abstract28]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP01-3.pdf">&#9658;</a></span></p>
  <h3><span lang=EN-GB style='mso-ansi-language:EN-GB'>Systems<o:p></o:p></span></h3>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>29.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Hugues Vinet (IRCAM), Perfecto Herrera (IA-UPF) and François
  Pachet (Sony CSL):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>The CUIDADO Project </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt'><a
  style='mso-comment-reference:Abstract_29'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_29"
  onmouseover="msoCommentShow('_anchor_29','_com_29')"
  onmouseout="msoCommentHide('_com_29')" href="#_msocom_29"
  language=JavaScript name="_msoanchor_29">[Abstract29]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP06-3.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>30.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Steffen Pauws (Philips Research <st1:City><st1:place>Eindhoven</st1:place></st1:City>):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>CubyHum: a fully
  operational ‚“query by humming“ system </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt'><a
  style='mso-comment-reference:Abstract_30'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_30"
  onmouseover="msoCommentShow('_anchor_30','_com_30')"
  onmouseout="msoCommentHide('_com_30')" href="#_msocom_30"
  language=JavaScript name="_msoanchor_30">[Abstract30]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP06-2.pdf">&#9658;</a></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>31.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Chaokun Wang, Jianzhong Li and Shengfei Shi (Harbin Institute of
  Technology):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>A Kind of Content-Based
  Music Information Retrieval Method in a Peer-to-Peer Environment </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt'><a
  style='mso-comment-reference:Abstract_31'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_31"
  onmouseover="msoCommentShow('_anchor_31','_com_31')"
  onmouseout="msoCommentHide('_com_31')" href="#_msocom_31"
  language=JavaScript name="_msoanchor_31">[Abstract31]</a><![endif]><span
  style='display:none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><b><span
  lang=EN-GB style='font-style:normal'> </span></b><span lang=EN-GB
  style='font-style:normal'><a href="proceedings/02-FP06-1.pdf">&#9658;</a></span></p>
  <p class=Paper style='margin-left:0mm;text-indent:0mm;mso-list:none;
  tab-stops:35.45pt'><![if !supportEmptyParas]>&nbsp;<![endif]><span
  lang=EN-GB style='font-size:12.0pt;font-style:normal'><o:p></o:p></span></p>
  </td>
 </tr>
 <tr style='mso-yfti-irow: 1;mso-yfti-lastrow: yes'>
  <td width=740 colspan=2 valign=top style='width:554.85pt;border:none;
  mso-border-top-alt:solid windowtext .5pt;background:#FFFFB3;padding:0mm 3.5pt 0mm 3.5pt'>
  <h1 align=center style='margin-top:0mm;text-align:center'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:16.0pt;mso-ansi-language:EN-GB'>The
  ISMIR 2002 Web pages will be regularly updated<br>
  to include program content and schedule<o:p></o:p></span></h1>
  </td>
 </tr>
</table>

</div>

<p class=MsoNormal><span lang=EN-GB style='mso-ansi-language:EN-GB'>&nbsp;<o:p></o:p></span></p>

</div>

<div style='mso-element:comment-list'><![if !supportAnnotations]>

<hr class=msocomoff align=left size=1 width="33%">

<![endif]>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_1" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_1','_com_1')"
onmouseout="msoCommentHide('_com_1')"><![endif]><span style='mso-comment-author:
Abstract'><![if !supportAnnotations]><a name="_msocom_1"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='mso-ansi-font-size:10.0pt;mso-ansi-language:EN-GB'><span
style="mso-spacerun: yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='mso-ansi-font-size:10.0pt'><span
style='mso-special-character:comment'>&nbsp;<![if !supportAnnotations]><a
href="#_msoanchor_1" class=msocomoff>[Abstract1]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-bidi-font-size:9.0pt;mso-ansi-language:EN-GB'>In most
popular music, the vocals sung of the lead singer are the focal point of the
song. The unique qualities of a singer’s voice make it relatively easy for us
to identify a song as belonging to that particular artist. With little
training, if one is familiar with a particular singer’s voice one can usually
recognize that voice in other pieces, even when hearing a song for the first
time. The research presented in this paper attempts to automatically establish
the identity of a singer using acoustic features extracted from songs in a
database of popular music. As a first step, an untrained algorithm for
automatically extracting vocal segments from within songs is presented. Once
these vocal segments are identified, they are presented to a singer
identification system that has been trained on data taken from other songs by
the same artists in the database.</span><span lang=EN-GB style='mso-ansi-language:
EN-GB'><o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_2" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_2','_com_2')"
onmouseout="msoCommentHide('_com_2')"><![endif]><span style='mso-comment-author:
Abstract'><![if !supportAnnotations]><a name="_msocom_2"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='mso-ansi-font-size:10.0pt;mso-ansi-language:EN-GB'><span
style="mso-spacerun: yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='mso-ansi-font-size:10.0pt'><span
style='mso-special-character:comment'>&nbsp;<![if !supportAnnotations]><a
href="#_msoanchor_2" class=msocomoff>[Abstract2]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-bidi-font-size:9.0pt;mso-ansi-language:EN-GB'>It would be
interesting and valuable to devise an automatic</span><span lang=EN-GB
style='mso-ansi-language:EN-GB'> </span><span lang=EN-GB style='mso-bidi-font-size:
9.0pt;mso-ansi-language:EN-GB'>measure of the similarity between two musicians
based only on an analysis of their recordings. To develop such a measure,
however, presupposes some ‘ground truth’ training data describing the actual
similarity between certain pairs of artists that constitute the desired output
of the measure. Since artist similarity is wholly subjective, such data is not
easily obtained. In this paper, we describe several attempts to construct a
full matrix of similarity measures between a set of some 400 popular artists by
regularizing limited subjective judgment data. We also detail our attempts to
evaluate these measures by comparison with some direct subjective similarity
judgments collected via a web-based survey in April 2002. Overall, we find that
subjective artist similarities are not consistent between users, undermining
the concept of a single ‘ground truth’, but we offer our best
common-denominator measures anyway.</span><span lang=EN-GB style='mso-ansi-language:
EN-GB'><o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_3" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_3','_com_3')"
onmouseout="msoCommentHide('_com_3')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_3"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='mso-ansi-font-size:10.0pt;mso-ansi-language:EN-GB'><span
style="mso-spacerun: yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='mso-ansi-font-size:10.0pt'><span
style='mso-special-character:comment'>&nbsp;<![if !supportAnnotations]><a
href="#_msoanchor_3" class=msocomoff>[Abstract3]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-bidi-font-size:9.0pt;mso-ansi-language:EN-GB'>A system is
described which measures the similarity of two arbitrary rhythmic patterns. The
patterns are represented as acoustic signals, and are not assumed to have been
performed with similar sound sets. Two novel methods are presented that
constitute the algorithmic core of the system. First, a probabilistic musical
meter estimation process is described, which segments a continuous musical
signal into patterns. As a side-product, the method outputs tatum, tactus
(beat), and measure lengths. A subsequent process performs the actual
similarity measurements. Acoustic features are extracted which model the
.uctuation of loudness and brightness withing the pattern, and dynamic time
warping is then applied to align the patterns to be compared. In simulations,
the system behaved consistently by assigning high similarity measures to
similar musical rhythms, even when performed using different sound sets.</span><span
lang=EN-GB style='mso-ansi-language:EN-GB'><o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_4" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_4','_com_4')"
onmouseout="msoCommentHide('_com_4')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_4"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='mso-ansi-font-size:10.0pt;mso-ansi-language:EN-GB'><span
style="mso-spacerun: yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='mso-ansi-font-size:10.0pt'><span
style='mso-special-character:comment'>&nbsp;<![if !supportAnnotations]><a
href="#_msoanchor_4" class=msocomoff>[Abstract4]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-bidi-font-size:8.5pt;mso-ansi-language:EN-GB'>Electronic<span
style="mso-spacerun: yes">  </span>Music<span style="mso-spacerun: yes"> 
</span>Distribution<span style="mso-spacerun: yes">  </span>(EMD)<span
style="mso-spacerun: yes">  </span>is<span style="mso-spacerun: yes"> 
</span>in<span style="mso-spacerun: yes">  </span>demand<span
style="mso-spacerun: yes">  </span>of<span style="mso-spacerun: yes"> 
</span>robust, automatically extracted music descriptors. We<span
style="mso-spacerun: yes">  </span>introduce a timbral similarity<span
style="mso-spacerun: yes">  </span>measures<span style="mso-spacerun: yes"> 
</span>for<span style="mso-spacerun: yes">  </span>comparing<span
style="mso-spacerun: yes">  </span>music<span style="mso-spacerun: yes"> 
</span>titles.<span style="mso-spacerun: yes">  </span>This<span
style="mso-spacerun: yes">  </span>measure<span style="mso-spacerun: yes"> 
</span>is based on a Gaussian model of cepstrum coefficients. We describe
the<span style="mso-spacerun: yes">  </span>timbre<span style="mso-spacerun:
yes">  </span>extractor<span style="mso-spacerun: yes">  </span>and<span
style="mso-spacerun: yes">  </span>the<span style="mso-spacerun: yes"> 
</span>corresponding<span style="mso-spacerun: yes">  </span>timbral<span
style="mso-spacerun: yes">  </span>similarity relation. We<span
style="mso-spacerun: yes">  </span>describe<span style="mso-spacerun: yes"> 
</span>experiments<span style="mso-spacerun: yes">  </span>in<span
style="mso-spacerun: yes">  </span>assessing<span style="mso-spacerun: yes"> 
</span>the<span style="mso-spacerun: yes">  </span>quality<span
style="mso-spacerun: yes">  </span>of<span style="mso-spacerun: yes"> 
</span>the similarity<span style="mso-spacerun: yes">  </span>relation,<span
style="mso-spacerun: yes">  </span>and<span style="mso-spacerun: yes"> 
</span>show<span style="mso-spacerun: yes">  </span>that<span
style="mso-spacerun: yes">  </span>the<span style="mso-spacerun: yes"> 
</span>measure<span style="mso-spacerun: yes">  </span>is<span
style="mso-spacerun: yes">  </span>able<span style="mso-spacerun: yes"> 
</span>to<span style="mso-spacerun: yes">  </span>yield interesting<span
style="mso-spacerun: yes">  </span>similarity<span style="mso-spacerun: yes"> 
</span>relations,<span style="mso-spacerun: yes">  </span>in<span
style="mso-spacerun: yes">  </span>particular<span style="mso-spacerun: yes"> 
</span>when<span style="mso-spacerun: yes">  </span>used<span
style="mso-spacerun: yes">  </span>in conjunction with other similarity
relations. We illustrate the use of the<span style="mso-spacerun: yes"> 
</span>descriptor<span style="mso-spacerun: yes">  </span>in<span
style="mso-spacerun: yes">  </span>several<span style="mso-spacerun: yes"> 
</span>EMD<span style="mso-spacerun: yes">  </span>applications<span
style="mso-spacerun: yes">  </span>developed<span style="mso-spacerun: yes"> 
</span>in<span style="mso-spacerun: yes">  </span>the </span><span lang=EN-GB
style='mso-ansi-language:EN-GB'>context of the Cuidado European project.<span
style="mso-spacerun: yes">   </span><o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_5" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_5','_com_5')"
onmouseout="msoCommentHide('_com_5')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_5"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='mso-ansi-font-size:10.0pt;mso-ansi-language:EN-GB'><span
style="mso-spacerun: yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='mso-ansi-font-size:10.0pt'><span
style='mso-special-character:comment'>&nbsp;<![if !supportAnnotations]><a
href="#_msoanchor_5" class=msocomoff>[Abstract5]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-bidi-font-size:9.0pt;mso-ansi-language:EN-GB'>This paper
presents a music summarization system called “Papipuun” that we are developing.
Papipuun performs quick listening in a manner similar to a stylus skipping on a
scratched record, but the skipping occurs correctly at punctuations of musical
phrases, not arbitrarily. First, we developed a method for representing
polyphony based on time-span reduction in the generative theory of tonal music
(GTTM) and the deductive object-oriented database (DOOD). The operation, least
upper bound, plays an important role in similarity checking of polyphonies
represented in our method. Next, in a preprocessing phase, a user analyzes a
set piece by the time-span reduction, using a dedicated tool, called TS-Editor.
For a real time phase, the user interacts with the main system, Summarizer, to
perform music summarization. Summarizer discovers a piece structure by
similarity checking. When the user identifies the fragments to be skipped,
Summarizer deletes them and concatenates the rest. Papipuun can produce the
music summarization of good quality, reflecting the atmosphere of an entire
piece through interaction with the user. </span><span lang=EN-GB
style='mso-ansi-language:EN-GB'><o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_6" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_6','_com_6')"
onmouseout="msoCommentHide('_com_6')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_6"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='mso-ansi-font-size:10.0pt;mso-ansi-language:EN-GB'><span
style="mso-spacerun: yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='mso-ansi-font-size:10.0pt'><span
style='mso-special-character:comment'>&nbsp;<![if !supportAnnotations]><a
href="#_msoanchor_6" class=msocomoff>[Abstract6]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-bidi-font-size:9.0pt;mso-ansi-language:EN-GB'>This paper
deals with the automatic generation of music audio summaries from signal
analysis without the use of any other information. The strategy employed here
is to consider the audio signal as a succession of “states” (at various scales)
corresponding to the structure (at various scales) of a piece of music. This
is, of course, only applicable to certain kinds of musical genres bas ed on
repetition.From the audio signal, we first derive features representing the
time evolution of the energy content in various frequency bands. These features
constitute our observations from which we derive a representation of the music
in terms of “states”. Since human segmentation and grouping performs better
upon subsequent hearings, this “natural” approach is followed here. The first
pass of the proposed algorithm uses segmentation in order to create “templates”
as “potential” states. The second pass uses these templates in order to
structure the music using unsupervised learning methods (k-means and hidden
Markov model). The audio summary is finally constructed by choosing a
representative example of each state. Further refinements of the summary audio
signal construction, uses overlap-add, and a tempo detection/ beat alignment in
order to improve the </span><span lang=EN-GB style='mso-ansi-language:EN-GB'>audio
quality of the created summary.</span><span lang=EN-GB style='mso-bidi-font-size:
9.0pt;mso-ansi-language:EN-GB'><o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_7" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_7','_com_7')"
onmouseout="msoCommentHide('_com_7')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_7"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='mso-ansi-font-size:10.0pt;mso-ansi-language:EN-GB'><span
style="mso-spacerun: yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='mso-ansi-font-size:10.0pt'><span
style='mso-special-character:comment'>&nbsp;<![if !supportAnnotations]><a
href="#_msoanchor_7" class=msocomoff>[Abstract7]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-bidi-font-size:9.0pt;mso-ansi-language:EN-GB'>We present
methods for automatically producing summary excerpts or thumbnails of music. To
find the most representative excerpt, we maximize the average segment
similarity to the entire work. After window-based audio parameterization, a
quantitative similarity measure is calculated between every pair of windows,
and the results are embedded in a 2-D similarity matrix. Summing the similarity
matrix over the support of a segment results in a measure of </span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>how similar that segment is to the
whole. This can be maximized to find the segment that best represents the
entire work. We discuss variations on the method, and present experimental
results for orchestral music, popular songs, and jazz. These results
demonstrate that the method finds significantly representative excerpts,
usingvery few assumptions about the source audio.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_8" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_8','_com_8')"
onmouseout="msoCommentHide('_com_8')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_8"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='mso-ansi-font-size:10.0pt;mso-ansi-language:EN-GB'><span
style="mso-spacerun: yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='mso-ansi-font-size:10.0pt'><span
style='mso-special-character:comment'>&nbsp;<![if !supportAnnotations]><a
href="#_msoanchor_8" class=msocomoff>[Abstract8]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-bidi-font-size:8.5pt;mso-ansi-language:EN-GB'>Human
listeners are able to recognize structure in music through the perception of
repetition and other relationships within a piece of music. This work aims to
automate the task of music analysis. Music is “explained” in terms of embedded
relationships, especially repetition of segments or phrases. The steps in this
process are the transcription of audio into a representation with a similarity
or distance metric, the search for similar segments, forming clusters of
similar segments, and explaining music in terms of these clusters. Several
transcription methods are considered: monophonic pitch estimation, chroma
(spectral) representation, and polyphonic transcription followed by harmonic analysis.
Also, several algorithms that search for similar segments are described. These
techniques can be used to perform an analysis of musical structure, as
illustrated by</span><span lang=EN-GB style='mso-ansi-language:EN-GB'>examples.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_9" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_9','_com_9')"
onmouseout="msoCommentHide('_com_9')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_9"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='mso-ansi-font-size:10.0pt;mso-ansi-language:EN-GB'><span
style="mso-spacerun: yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='mso-ansi-font-size:10.0pt'><span
style='mso-special-character:comment'>&nbsp;<![if !supportAnnotations]><a
href="#_msoanchor_9" class=msocomoff>[Abstract9]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>We present an efficient and scalable
system that indexes acoustic music data for content-based music retrieval. Both
the music database and input queries are given in raw audio formats without
metadata or other symbolic information; retrieval is targeted at music pieces
which are “similar” to the query sound clip. Our framework is designed as a
series of modular pipeline stages and phases. Each music file entering the
pipeline is transformed into spectogram vectors and then into <i>characteristic
sequences</i>,<i> </i>representing small segments of audio features than can
tolerate some noise and tempo variations. These sequences are placed in a
high-dimensional indexing structure. Retrieval results from the index are
ranked based on alignment of short matching segments. Each module of the
framework can be independently changed or replaced, and their effect are
studied by experiments.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_10" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_10','_com_10')"
onmouseout="msoCommentHide('_com_10')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_10"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
lang=EN-GB style='mso-ansi-language:EN-GB'>With the advent of large musical
archives the need to provide an organization of these archives becomes eminent.
While artist-based organizations or title indexes may help in locating a
specific piece of music, a more intuitive, genre-based organization is required
to allow users to browse an archive and explore its contents. Yet, </span><span
lang=EN-GB style='mso-bidi-font-size:9.0pt;mso-ansi-language:EN-GB'>currently</span><span
lang=EN-GB style='mso-ansi-language:EN-GB'> these organizations following
musical styles have to be designed manually. In this paper we propose an
approach to automatically create a hierarchical organization of music archives
following their perceived sound similarity. More specifically, characteristics
of frequency spectra are extracted and transformed according to psycho-acoustic
models. Subsequently, the Growing Hierarchical Self-Organizing Map, a popular
unsupervised neural network, is used to reate a hierarchical organization,
o®ering both an interface for interactive exploration as well as retrieval of
music according to sound similarity.<o:p></o:p></span></p>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='mso-ansi-font-size:10.0pt;mso-ansi-language:EN-GB'><span
style="mso-spacerun: yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='mso-ansi-font-size:10.0pt'><span
style='mso-special-character:comment'>&nbsp;<![if !supportAnnotations]><a
href="#_msoanchor_10" class=msocomoff>[Abstract10]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'><o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_11" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_11','_com_11')"
onmouseout="msoCommentHide('_com_11')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_11"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_11"
class=msocomoff>[Abstract11]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>In this paper we present a musical
style identification scheme based on simultaneous classification of auditory
and textual data. Style identification is a task which often involves cultural
aspects not present or easily extracted through auditory processing. The scheme
we propose complements any audio driven genre or style detection system with a
classifier based on web-extracted data we call &quot;community metadata.&quot;
The addition of these cultural attributes in our feature space aids in proper
classification of acoustically dissimilar music within the same style, and
similar music belonging to different styles.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_12" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_12','_com_12')"
onmouseout="msoCommentHide('_com_12')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_12"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_12"
class=msocomoff>[Abstract12]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>Much research has been published on
musical taste, however, little has been studied by the builders of music recommenders.
Implicit and explicit collaborative filtering has been used for making
recommenders, in addition to the automatic classification of music into style
categories based on extracted audio features. This paper surveys research into
musical taste, reviews music recommender research, and outlines promising
directions. In particular, we learned that demographic and personality factors
have been shown to be factors influencing music preference. For mood, the main
factors are tempo, tonality, distinctiveness of rhythm and pitch height.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_13" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_13','_com_13')"
onmouseout="msoCommentHide('_com_13')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_13"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_13"
class=msocomoff>[Abstract13]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>A means to ease selecting preferred
music referred to as Personalized Automatic Track Selection (PATS) has been
developed. PATS generates playlists that suit a particular context-of-use, that
is, the real-world environment in which the music is heard. To create
playlists, it uses a dynamic clustering method in which songs are grouped based
on their attribute similarity. The similarity measure selectively weighs
attribute-values, as not all attribute-values for a context-of-use from
preference feedback of the user. In a controlled user experiment, the quality
of PATS-compiled and randomly assembled playlists for jazz music was assessed
in two contexts-of-use. The quality of the randomly assembled playlists was
used as base-line. The two contexts-of-use were &quot;listening to soft
music&quot; and &quot;listening to lively music&quot;. Playlist quality was
measured by <i>precision </i>(songs that suit the context-of-use), <i>coverage </i>(songs
that suit the context-of-use but that were not already contained in previous
playlists) and a <i>rating score</i>. Results showed that PATS playlists
contained increasingly more preferred music (increasingly higher <i>precision</i>),
covered more preferred music in the collection (higher <i>coverage</i>), and
were <i>rated </i>higher than randomly assembled playlists.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_14" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_14','_com_14')"
onmouseout="msoCommentHide('_com_14')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_14"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='mso-ansi-font-size:10.0pt;mso-bidi-font-size:
10.0pt'><span style='mso-special-character:comment'>&nbsp;<![if !supportAnnotations]><a
href="#_msoanchor_14" class=msocomoff>[Abstract14]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>Music Information Retrieval (MIR) is
an interdisciplinary research area that has grown out of need to manage
burgeoning collections of music in digital form. Its diverse disciplinary
communities have yet to articulate a common research agenda or agree on method<span
class=CommentaireCar><span style='mso-ansi-language:EN-GB'>ological </span></span>principles
and metrics of success. In order for MIR to succeed, researchers need to work
with real user communities and develop research resources such as reference
music collections, so that the wide variety of techniques being developed in
MIR can be meaningfully compared with one another. Out of these efforts, a
common MIR practice can emerge. <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_15" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_15','_com_15')"
onmouseout="msoCommentHide('_com_15')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_15"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:9.0pt;font-family:TimesNewRomanPSMT'><span
style='mso-special-character:comment'>&nbsp;<![if !supportAnnotations]><a
href="#_msoanchor_15" class=msocomoff>[Abstract15]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>There has been substantial research
on technical aspects of musical digital libraries, but comparatively little on
usability aspects. We have evaluated four web-accessible music libraries,
focusing particularly on features that are particular to music libraries, such
as music retrieval mechanisms. Although the original focus of the work was on
how modalities are combined within the interactions with such libraries, that
was not where the main difficulties were found. Libraries were generally well
designed for use of different modalities. The main challenges identified relate
to the details of melody matching and to simplifying the choices of file
format. These issues are discussed in detail. <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_16" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_16','_com_16')"
onmouseout="msoCommentHide('_com_16')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_16"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='mso-ansi-font-size:10.0pt;mso-bidi-font-size:
10.0pt'><span style='mso-special-character:comment'>&nbsp;<![if !supportAnnotations]><a
href="#_msoanchor_16" class=msocomoff>[Abstract16]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>Previous research has demonstrated
that people listen to music for various reasons. The purpose of this study was
to investigate people’s perception of music, and thus their music information
needs. These ideas were examined by presenting 22 participants with 7 classical
musical pieces, asking one-half of them to write words descriptive of each
piece, and the other half words they would use if searching for each piece. All
the words used by all subjects in both tasks were classified into 7 categories.
The two most frequently appearing categories were <i>emotions </i>and <i>occasions
or filmed events </i>regardless of the task type. These subjects, none of whom
had formal training in music, almost never used words related to formal
features of music, rather almost always using words indicating other features,
most of which have not been considered in existing or proposed music IR
systems. These results suggest that music IR research should be extended to
consider needs other than finding known items, or items identified by formal
characteristics, and that understanding music information needs of users should
be prioritized to design more sophisticated music IR systems. <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_17" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_17','_com_17')"
onmouseout="msoCommentHide('_com_17')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_17"></a><![endif]></span>

<p class=MsoNormal style='text-align:justify;text-justify:inter-ideograph;
mso-layout-grid-align:none'><span class=MsoCommentReference><span
style='font-size:10.0pt'><span style='mso-special-character:comment'>&nbsp;<![if !supportAnnotations]><a
href="#_msoanchor_17" class=msocomoff>[Abstract17]</a><![endif]></span></span></span><span
lang=EN-GB style='font-size:10.0pt;mso-ansi-language:EN-GB'>Imagine the
following situation. You’re in your car, listening to the radio and suddenly
you hear a song that catches your attention. It’s the best new song you have
heard for a long time, but you missed the announcement and don’t recognize the
artist. Still, you would like to know more about this music. What should you
do? You could call the radio station, but that’s too cumbersome. Wouldn’t it be
nice if you could push a few buttons on your mobile phone and a few seconds
later the phone would respond with the name of the artist and the title of the
music you’re listening to? Perhaps even sending an email to your default email
address with some supplemental information. In this paper we present an audio
fingerprinting system, which makes the above scenario possible. By using the
fingerprint of an unknown audio clip as a query on a fingerprint database,
which contains the fingerprints of a large library of songs, the audio clip can
be identified. At the core of the presented system are a highly robust
fingerprint extraction method and a very efficient fingerprint search strategy,
which enables searching a large fingerprint database with only limited
computing resources. <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_18" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_18','_com_18')"
onmouseout="msoCommentHide('_com_18')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_18"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
lang=EN-GB style='mso-ansi-language:EN-GB'>This paper extends the familiar
&quot;query by humming&quot; music retrieval framework into the polyphonic
realm. As humming in multiple voices is quite difficult, the task is more
accurately described as &quot;query by audio example&quot;, onto a collection
of scores. To our knowledge, we are the first to use polyphonic symbolic
collections. Furthermore, as our results will show, we will not only use an
audio query to retrieve a known-item symbolic piece, but we will use it to
retrieve an entire set of real-world composed variations on that piece, also in
the symbolic format. The harmonic modeling approach which forms the basis of
this work is a new and valuable technique which as both wide-applicability and
long-range future potential.</span><span class=MsoCommentReference><span
style='mso-ansi-font-size:10.0pt'><span style='mso-special-character:comment'>&nbsp;<![if !supportAnnotations]><a
href="#_msoanchor_18" class=msocomoff>[Abstract18]</a><![endif]></span></span></span><span
style='mso-ansi-language:EN-GB'> <span lang=EN-GB><o:p></o:p></span></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_19" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_19','_com_19')"
onmouseout="msoCommentHide('_com_19')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_19"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_19"
class=msocomoff>[Abstract19]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>Recently a great attention is paid
to content-based multimedia retrieval that enables users to find and locate
audio-visual materials according to the intrinsic characteristics of the
target. Query by humming (QBH) is also an application that makes retrieval
according to major characteristics of music, that is, &quot;melody&quot;. There
are couples of researches on QBH system, but their major concern is the system
that retrieves symbolic music data by humming query. But when the usability of
technology is taken into consideration, retrieval of music in the form of
polyphonic audio would be more useful and needed in the application such as
internet music search or music juke box, where the music data is stored not in
symbolic form but in raw audio signal because such music data is more natural
format for consumption. Our focus is on the realization of query-byhumming
technology to easy-to-use application, which entails full automation of all the
processes of the system, including melody information extraction from
polyphonic audio. Melody feature of music and humming is not represented by
distinct note information but the possibilities of note occurrence. Similarity is
then measured between the melody features of humming and music using DP
matching method. This paper presents developed algorithms for key steps of QBH
system including the melody feature extraction method from polyphonic audio and
humming, their representation for matching, and matching method between
represented melody information from polyphonic audio and humming. <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_20" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_20','_com_20')"
onmouseout="msoCommentHide('_com_20')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_20"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_20"
class=msocomoff>[Abstract20]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>Many of the large digital music
collections available today are in polyphonic form. However, because of the
complexities of music information retrieval (Music IR), much of the research in
this area has focused on monophonic data. In this paper we investigate the
retrieval performance of monophonic queries made on a polyphonic music database
using the n-gram approach for full-music indexing. The pitch and rhythm
dimensions of music are used and the ‘musical words’ generated enable text
retrieval methods to be used with music retrieval. An experimental framework is
outlined for a comparative and fault-tolerance study of various n-gramming strategies
and encoding precision using six experimental databases. For monophonic queries
we focus in particular on query-by-humming (QBH) systems. Error models
addressed in several QBH studies are surveyed for the faulttolerance study. The
experiments show that different n-gramming strategies and encoding precision
differ widely in their effectiveness. We present the results of our comparative
and fault-tolerance study on a collection of 6365 polyphonic music pieces
encoded in the <st1:place>MIDI</st1:place> format. <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_21" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_21','_com_21')"
onmouseout="msoCommentHide('_com_21')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_21"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_21"
class=msocomoff>[Abstract21]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>We propose a model for errors in
sung queries, a variant of the Hidden Markov Model (hmm). This is related to
the problem of identifying the degree of similarity between a <i>query </i>and
a potential <i>target </i>in a database of musical works, in the music
retrieval framework. The model comprehensively expresses the types of <i>error </i>or
variation between target and query: cumulative and non-cumulative local errors,
transposition, tempo and tempo changes, insertions, deletions and modulation.
Results of experiments demonstrating the robustness of such a model are
presented. <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_22" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_22','_com_22')"
onmouseout="msoCommentHide('_com_22')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_22"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
lang=EN-GB style='mso-ansi-language:EN-GB'>In this paper, a new system for the
automatic transcription of singing sequences into a sequence of pitch and
duration pairs is presented. Although such a system may have a wider range of
applications, it was mainly developed to become the acoustic module of a
query-by-humming (QBH) system for retrieving pieces of music from a digitized
musical library. The first part of the paper is devoted to the systematic
evaluation of a variety of state-of-the art transcription systems. The main
result of this evaluation is that there is clearly a need for more accurate
systems. Especially the segmentation was experienced as being too error prone
(&#8776; 20</span><span lang=EN-GB style='font-size:1.0pt;font-family:Arial;
mso-ansi-language:EN-GB'>_ </span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>% segmentation errors). In the second part of the paper, a new auditory
model based transcription system is proposed and evaluated. The results of that
evaluation are very promising. Segmentation errors vary between 0 and 7 %,
depending on the amount of lyrics that is used by the singer. Anyway, an error
of less than 10 % is anticipated to be acceptable for QBH. The paper ends with
the description of an experimental study that was issued to demonstrate that
the accuracy of the newly proposed transcription system is not very sensitive to
the choice of the free parameters, at least as long as they remain in the
vicinity of the values one could forecast on the basis of their meaning.<span
style="mso-spacerun: yes">     </span><o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_23" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_23','_com_23')"
onmouseout="msoCommentHide('_com_23')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_23"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_23"
class=msocomoff>[Abstract23]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>A hidden Markov model approach to
piano music transcription is presented. The main difficulty in applying
traditional HMM techniques is the large number of chord hypotheses that must be
considered. We address this problem by using a trained likelihood model to
generate reasonable hypotheses for each frame and construct the search graph
out of these hypotheses. Results are presented using a recording of a movement
from Mozart's Sonata 18, K. 570. <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_24" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_24','_com_24')"
onmouseout="msoCommentHide('_com_24')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_24"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_24"
class=msocomoff>[Abstract24]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>Voice separation, along with
tempo-detection and quantization, is one of the basic problems of
computer-based transcription of music. An adequate separation of notes into
different voices is crucial for obtaining readable and usable scores from
performances of polyphonic music recorded on keyboard (or other polyphonic)
instruments; for improving quantisation results within a transcription system;
and in the context of music retrieval systems that primarily support monophonic
queries. In this paper we propose a new voice separation algorithm based on a
stochastic local search method. Different from many previous approaches, our
algorithm allows chors in the individual voices; its behaviour is controlled by
a small number of intuitive and musically motivated parameters; and it is fast
enough to allow interactive optimisation of the result by adjusting the
parameters in real-time. We demonstrate that compared to existing approaches,
our new algorithm generates better solutions for a number of typical voice
separation problems. We also show how by changing its parameters it is possible
to create score output for different needs (i.e. piano-style or orchestral
scores). <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_25" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_25','_com_25')"
onmouseout="msoCommentHide('_com_25')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_25"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_25"
class=msocomoff>[Abstract25]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>The Music Information Retrieval
methods can be classified into online and offline methods. The main drawback in
most of the offline algorithms is the space the indexing structure requires.
The amount of the data stored into the structure can however be reduced by
storing only the suitable index terms or phrases instead of the whole contents
of the database. Repetition is agreed to be one of the most important factors
of musical meaningfulness. Therefore repetitive phrases are suitable for
indexing purposes. The extraction of such phrases can be done by applying and
existing text mining method to musical data. Because of the differences between
text and musical data the application requires some technical modification of
the method. This paper introduces a text mining-based music database indexing
method that extracts maximal frequent phrases from musical data and sorts them
by their length, frequency and personality. The implementation of the method
found three different types of phrases from the test corpus consisting of Irish
folk music tunes. The suitable two types of phrases out of three are easily
recognized and separated from the set of all phrases to form an index data for
the database. <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_26" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_26','_com_26')"
onmouseout="msoCommentHide('_com_26')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_26"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_26"
class=msocomoff>[Abstract26]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>In order to represent musical
content, pitch and timing information is utilized in the majority of existing
work in Symbolic Music Information Retrieval (MIR). Symbolic representations
such as <st1:place>MIDI</st1:place> allow the easy calculation of such
information and its manipulation. In contrast, most of the existing work in
Audio MIR uses timbral and beat information, which can be calculated using
automatic computer audition techniques. In this paper, Pitch Histograms are
defined and proposed as a way to represent the pitch content of music signals
both in symbolic and audio form. This representation is evaluated in the
context of automatic musical genre classification. A multiple-pitch detection
algorithm for polyphonic signals is used to calculate Pitch Histograms for
audio signals. In order to evaluate the extent and significance of errors
resulting from the automatic multiple-pitch detection, automatic musical genre
classification results from symbolic and audio data are compared. The
comparison indicates that Pitch Histograms provide valuable information for
musical genre classification. The results obtained for both symbolic and audio
cases indicate that although pitch errors degrade classification performance
for the audio case, Pitch Histograms can be effectively used for classification
in both cases. <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_27" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_27','_com_27')"
onmouseout="msoCommentHide('_com_27')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_27"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_27"
class=msocomoff>[Abstract27]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>The main contribution of this paper
is an invistigation on the effects of exploiting melodic features for automatic
melody segmentation aimed at content-based usicd retrieval. We argue that
segmentation based on melodic features is more effective than random or
n-grams-based segmentation, which ignore any context. We have carried out an
experiment employing experienced subjects. The manual segmentation result has
been processed to detect the most probably boundaries in the melodic surface,
using a probabilistic decision function. The detected boundaries have then been
compared with the boundaries detected by an automatic precedure implementing an
algorithm for melody segmentation, as well as by a random segmenter and by a
n-gram-based segmenter. Results showed that automatic segmentation based on
melodic features is closer to manual segmentation that algorithms that do not
use such information. <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_28" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_28','_com_28')"
onmouseout="msoCommentHide('_com_28')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_28"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_28"
class=msocomoff>[Abstract28]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>Hidden Markov Models (HMMs) have
been suggested as an effective technique to represent music. Given a collection
of musical pieces, each represented by its HMM, and a query , the retrieval
task reduces to finding HMM most likely to have generated the query. The
musical piece represented by this HMM is frequently the one rendered by the
user, possibly imperfectly. This method might be inefficient if there is a very
large music database, since each HMM to be tested requires the evaluation of a
dynamic-programming algorithm. In this paper, we propose an indexing mechanism
that can aggressively prune the set of condidiate HMMs to be evaluated in response
to a query. Our experiments on a music database showed an anverage of a
seven-fold spped up with no false dismissals. <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_29" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_29','_com_29')"
onmouseout="msoCommentHide('_com_29')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_29"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_29"
class=msocomoff>[Abstract29]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>The CUIDADO Project (Content-based
Unified Interfaces and Descriptors for Audio/music Databases available Online)
aims at developing a new chain of applications through the use of audio/music
content descriptors, in the spirit of the MPEG-7 standard. The project includes
the design of appropriate description structures, the development of extractors
for deriving high-level information from audio signals, and the design and
implementation of two applications: the Sound Palette and the Music Browser.
These applications include new features which systematically exploit high-level
descriptors and provide users with content-based access to large catalogues of
audio/music material. The Sound Palette is focused on audio samples and targets
professional users, whereas the Music Browser addresses a broader user target
through the management of music titles. After a presentation of the project
objectives and methodology, we describe the original features of the two
applications made possible by the use of descriptors and the technical
architecture framework on which they rely. <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_30" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_30','_com_30')"
onmouseout="msoCommentHide('_com_30')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_30"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_30"
class=msocomoff>[Abstract30]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>&quot;Query by humming&quot; is an
interaction concept in which the identity of a song has to be revealed fast and
orderly from a given sung input using a large database of known melodies. In
short, it tries to detect the pitches in a sung melody and compares these
pitches with symbolic representations of the known melodies. Melodies that are
similar to the sung pitches are retreved. Approximate pattern matching in the
melody comparison process compensates for the errors in the sung melody by
using classical dynamic programming. A filtering method is use to save
computation in the dynamic programming framework. This paper presents the
algorithms for pitch detection, note onset detection, quantization, melody
encoding and approximate pattern matching as they have been implemented in the
Cubyllum software system. Since human reproduction of melodies is imperfect,
findings from an experimental singing study were a crucial input to the
development of the algorithms. Future research should pay special attention to
the reliable detection of note onsets in any preferred singing style. in addition,
research on index methods and fast bit-parallelism algorithms for approximate
pattern matching needs to be further pursued to decrease computational
requirements when dealing with large melody databases. <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_31" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_31','_com_31')"
onmouseout="msoCommentHide('_com_31')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_31"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_31"
class=msocomoff>[Abstract31]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>In this paper, we propose four
peer-to-peer models for content-based music information retrieval (CBMIR) and
carefully evaluate them on load, time, refreshment and robustness qualitatively
and quantitatively. And we bring forward an algorithm to accelerate the
retrieval speed of CBP2PMIR and a simple but effective method to filter the
replica in the final results. And we present the architecture of content-based
peer-to-peer music information retrieval system QUIND which can implement
CBMIR. QUIND combines content-based music information retrieval technologies
and peer-to-peer environment, and has good robustness and expansibility. Music
stored and shared on each PC makes up of the whole available music resource.
When user puts forward a music query, e.g. a song or a melody, QUIND can
retrieve a lot of similar music quickly and accurately according to the content
of query music. After user selects his favorite ones, he can download and enjoy
them. <o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

</div>

</body>


<!-- Mirrored from ismir2002.ismir.net/papers.html by HTTrack Website Copier/3.x [XR&CO'2013], Thu, 23 Jan 2014 09:20:41 GMT -->
</html>
