<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:w="urn:schemas-microsoft-com:office:word"
xmlns="http://www.w3.org/TR/REC-html40">


<!-- Mirrored from ismir2002.ismir.net/posters-fr.html by HTTrack Website Copier/3.x [XR&CO'2013], Thu, 23 Jan 2014 11:32:09 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=windows-1252" /><!-- /Added by HTTrack -->
<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=ProgId content=Word.Document>
<meta name=Generator content="Microsoft Word 9">
<meta name=Originator content="Microsoft Word 9">
<link rel=File-List href="posters-fr_fichiers/filelist.xml">
<link rel=Edit-Time-Data href="posters-fr_fichiers/editdata.html">
<link rel=OLE-Object-Data href="posters-fr_fichiers/oledata.mso">
<!--[if !mso]>
<style>
v\:* {behavior:url(#default#VML);}
o\:* {behavior:url(#default#VML);}
w\:* {behavior:url(#default#VML);}
.shape {behavior:url(#default#VML);}
</style>
<![endif]-->
<title>ISMIR 2002 Panneaux/articles courts</title>
<!--[if gte mso 9]><xml>
 <o:DocumentProperties>
  <o:Author>Michael Fingerhut</o:Author>
  <o:LastAuthor>Michael Fingerhut</o:LastAuthor>
  <o:Revision>7</o:Revision>
  <o:TotalTime>282</o:TotalTime>
  <o:Created>2002-07-08T08:25:00Z</o:Created>
  <o:LastSaved>2002-09-10T12:33:00Z</o:LastSaved>
  <o:Pages>2</o:Pages>
  <o:Words>769</o:Words>
  <o:Characters>4388</o:Characters>
  <o:Company>Ircam - Centre Pompidou</o:Company>
  <o:Lines>36</o:Lines>
  <o:Paragraphs>8</o:Paragraphs>
  <o:CharactersWithSpaces>5388</o:CharactersWithSpaces>
  <o:Version>9.4402</o:Version>
 </o:DocumentProperties>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <w:WordDocument>
  <w:Zoom>120</w:Zoom>
  <w:HyphenationZone>21</w:HyphenationZone>
 </w:WordDocument>
</xml><![endif]--><![if !supportAnnotations]>
<style id="dynCom" type="text/css"><!-- --></style>
<script language="JavaScript"><!--
function msoCommentShow(anchor_id, com_id)
{
	if(msoBrowserCheck()) 
		{
		c = document.all(com_id);
		if (null != c)
			{
			a = document.all(anchor_id);
			var cw = c.offsetWidth;
			var ch = c.offsetHeight;
			var aw = a.offsetWidth;
			var ah = a.offsetHeight;
			var x  = a.offsetLeft;
			var y  = a.offsetTop;
			var el = a;
			while (el.tagName != "BODY") 
				{
				el = el.offsetParent;
				x = x + el.offsetLeft;
				y = y + el.offsetTop;
				}
			var bw = document.body.clientWidth;
			var bh = document.body.clientHeight;
			var bsl = document.body.scrollLeft;
			var bst = document.body.scrollTop;
			if (x + cw + ah / 2 > bw + bsl && x + aw - ah / 2 - cw >= bsl ) 
				{ c.style.left = x + aw - ah / 2 - cw; }
			else 
				{ c.style.left = x + ah / 2; }
			if (y + ch + ah / 2 > bh + bst && y + ah / 2 - ch >= bst ) 
				{ c.style.top = y + ah / 2 - ch; }
			else 
				{ c.style.top = y + ah / 2; }
			c.style.visibility = "visible";
}	}	}
function msoCommentHide(com_id) 
{
	if(msoBrowserCheck())
		{
		c = document.all(com_id);
		if (null != c)
		{
		c.style.visibility = "hidden";
		c.style.left = -1000;
		c.style.top = -1000;
		} } 
}
function msoBrowserCheck()
{
	ms = navigator.appVersion.indexOf("MSIE");
	vers = navigator.appVersion.substring(ms + 5, ms + 6);
	ie4 = (ms > 0) && (parseInt(vers) >= 4);
	return ie4;
}
if (msoBrowserCheck())
{
	document.styleSheets.dynCom.addRule(".msocomanchor","background: infobackground");
	document.styleSheets.dynCom.addRule(".msocomoff","display: none");
	document.styleSheets.dynCom.addRule(".msocomtxt","visibility: hidden");
	document.styleSheets.dynCom.addRule(".msocomtxt","position: absolute");
	document.styleSheets.dynCom.addRule(".msocomtxt","top: -1000");
	document.styleSheets.dynCom.addRule(".msocomtxt","left: -1000");
	document.styleSheets.dynCom.addRule(".msocomtxt","width: 33%");
	document.styleSheets.dynCom.addRule(".msocomtxt","background: infobackground");
	document.styleSheets.dynCom.addRule(".msocomtxt","color: infotext");
	document.styleSheets.dynCom.addRule(".msocomtxt","border-top: 1pt solid threedlightshadow");
	document.styleSheets.dynCom.addRule(".msocomtxt","border-right: 2pt solid threedshadow");
	document.styleSheets.dynCom.addRule(".msocomtxt","border-bottom: 2pt solid threedshadow");
	document.styleSheets.dynCom.addRule(".msocomtxt","border-left: 1pt solid threedlightshadow");
	document.styleSheets.dynCom.addRule(".msocomtxt","padding: 3pt 3pt 3pt 3pt");
}
// --></script>
<![endif]>
<style>
<!--
 /* Font Definitions */
@font-face
	{font-family:"Arial Unicode MS";
	panose-1:2 11 6 4 2 2 2 2 2 4;
	mso-font-charset:128;
	mso-generic-font-family:swiss;
	mso-font-pitch:variable;
	mso-font-signature:-1 -369098753 63 0 4129023 0;}
@font-face
	{font-family:"\@Arial Unicode MS";
	mso-font-charset:128;
	mso-generic-font-family:swiss;
	mso-font-pitch:variable;
	mso-font-signature:-1 -369098753 63 0 4129023 0;}
 /* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{mso-style-parent:"";
	margin:0cm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
h1
	{mso-style-next:Normal;
	margin-top:12.0pt;
	margin-right:0cm;
	margin-bottom:3.0pt;
	margin-left:0cm;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	mso-outline-level:1;
	font-size:16.0pt;
	font-family:Arial;
	mso-font-kerning:16.0pt;}
h2
	{mso-style-next:Normal;
	margin-top:12.0pt;
	margin-right:0cm;
	margin-bottom:3.0pt;
	margin-left:0cm;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	mso-outline-level:2;
	font-size:14.0pt;
	font-family:Arial;
	font-style:italic;}
h3
	{margin-right:0cm;
	mso-margin-top-alt:auto;
	mso-margin-bottom-alt:auto;
	margin-left:0cm;
	mso-pagination:widow-orphan;
	mso-outline-level:3;
	font-size:13.5pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Arial Unicode MS";}
h4
	{mso-style-next:Normal;
	margin:0cm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	mso-outline-level:4;
	tab-stops:108.0pt;
	font-size:10.0pt;
	mso-bidi-font-size:12.0pt;
	font-family:Arial;
	mso-ansi-language:EN-GB;}
h5
	{mso-style-next:Normal;
	margin:0cm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	mso-outline-level:5;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-ansi-language:EN-GB;}
h6
	{mso-style-next:Normal;
	margin-top:0cm;
	margin-right:0cm;
	margin-bottom:0cm;
	margin-left:28.4pt;
	margin-bottom:.0001pt;
	text-indent:-14.2pt;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	mso-outline-level:6;
	tab-stops:14.2pt;
	font-size:10.0pt;
	mso-bidi-font-size:12.0pt;
	font-family:Arial;}
p.MsoHeading7, li.MsoHeading7, div.MsoHeading7
	{mso-style-next:Normal;
	margin:0cm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	page-break-after:avoid;
	mso-outline-level:7;
	font-size:10.0pt;
	mso-bidi-font-size:12.0pt;
	font-family:Arial;
	mso-fareast-font-family:"Times New Roman";
	mso-ansi-language:EN-GB;
	font-weight:bold;}
p.MsoCommentText, li.MsoCommentText, div.MsoCommentText
	{margin:0cm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:10.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
span.MsoCommentReference
	{mso-ansi-font-size:8.0pt;
	mso-bidi-font-size:8.0pt;}
p.MsoListBullet, li.MsoListBullet, div.MsoListBullet
	{mso-style-update:auto;
	margin-top:0cm;
	margin-right:0cm;
	margin-bottom:0cm;
	margin-left:28.5pt;
	margin-bottom:.0001pt;
	text-indent:-18.0pt;
	mso-pagination:widow-orphan;
	mso-list:l1 level1 lfo3;
	tab-stops:list 28.5pt;
	font-size:10.0pt;
	mso-bidi-font-size:12.0pt;
	font-family:Arial;
	mso-fareast-font-family:"Times New Roman";
	mso-ansi-language:EN-GB;}
p.MsoListNumber, li.MsoListNumber, div.MsoListNumber
	{margin-top:0cm;
	margin-right:0cm;
	margin-bottom:0cm;
	margin-left:18.0pt;
	margin-bottom:.0001pt;
	text-indent:-18.0pt;
	mso-pagination:widow-orphan;
	mso-list:l0 level1 lfo6;
	tab-stops:list 18.0pt;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
p.MsoBodyText, li.MsoBodyText, div.MsoBodyText
	{margin-top:0cm;
	margin-right:0cm;
	margin-bottom:6.0pt;
	margin-left:0cm;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
p.MsoBodyTextFirstIndent, li.MsoBodyTextFirstIndent, div.MsoBodyTextFirstIndent
	{mso-style-parent:"Corps de texte";
	margin-top:0cm;
	margin-right:0cm;
	margin-bottom:6.0pt;
	margin-left:0cm;
	text-indent:10.5pt;
	mso-pagination:widow-orphan;
	font-size:10.0pt;
	mso-bidi-font-size:12.0pt;
	font-family:Arial;
	mso-fareast-font-family:"Times New Roman";
	mso-ansi-language:EN-GB;}
a:link, span.MsoHyperlink
	{color:blue;
	text-decoration:underline;
	text-underline:single;}
a:visited, span.MsoHyperlinkFollowed
	{color:purple;
	text-decoration:underline;
	text-underline:single;}
p.Paper, li.Paper, div.Paper
	{mso-style-name:Paper;
	mso-style-parent:"Liste à numéros";
	margin-top:0cm;
	margin-right:0cm;
	margin-bottom:3.0pt;
	margin-left:17.85pt;
	text-indent:-17.85pt;
	mso-pagination:widow-orphan;
	mso-list:l0 level1 lfo6;
	tab-stops:list 18.0pt;
	font-size:10.0pt;
	mso-bidi-font-size:12.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";
	mso-ansi-language:EN-GB;
	font-style:italic;}
@page Section1
	{size:595.3pt 841.9pt;
	margin:70.9pt 70.9pt 70.9pt 70.9pt;
	mso-header-margin:35.45pt;
	mso-footer-margin:35.45pt;
	mso-paper-source:0;}
div.Section1
	{page:Section1;}
 /* List Definitions */
@list l0
	{mso-list-id:-120;
	mso-list-type:simple;
	mso-list-template-ids:-943046792;}
@list l0:level1
	{mso-level-style-link:"Liste à numéros";
	mso-level-tab-stop:18.0pt;
	mso-level-number-position:left;
	margin-left:18.0pt;
	text-indent:-18.0pt;}
@list l1
	{mso-list-id:-119;
	mso-list-type:simple;
	mso-list-template-ids:-642188706;}
@list l1:level1
	{mso-level-number-format:bullet;
	mso-level-style-link:"Liste à puces";
	mso-level-text:\F0B7;
	mso-level-tab-stop:18.0pt;
	mso-level-number-position:left;
	margin-left:18.0pt;
	text-indent:-18.0pt;
	font-family:Symbol;}
ol
	{margin-bottom:0cm;}
ul
	{margin-bottom:0cm;}
-->
</style>
<!--[if gte mso 9]><xml>
 <o:shapedefaults v:ext="edit" spidmax="1033">
  <o:colormru v:ext="edit" colors="#ffc"/>
  <o:colormenu v:ext="edit" fillcolor="#ffc"/>
 </o:shapedefaults></xml><![endif]--><!--[if gte mso 9]><xml>
 <o:shapelayout v:ext="edit">
  <o:idmap v:ext="edit" data="1"/>
 </o:shapelayout></xml><![endif]-->
</head>

<body bgcolor="#ffffcc" lang=FR link=blue vlink=purple style='tab-interval:
35.45pt'>

<div class=Section1>

<div align=center>

<table border=0 cellspacing=0 cellpadding=0 width="90%" style='width:90.06%;
 margin-left:-.4pt;border-collapse:collapse;mso-padding-alt:0cm 3.5pt 0cm 3.5pt'>
 <tr>
  <td width=203 valign=top style='width:151.9pt;border-top:none;border-left:
  none;border-bottom:solid windowtext .5pt;border-right:solid windowtext .5pt;
  background:#FFFFB3;padding:0cm 3.5pt 0cm 3.5pt'>
  <p class=MsoNormal><!--[if gte vml 1]><v:shapetype id="_x0000_t75"
   coordsize="21600,21600" o:spt="75" o:preferrelative="t" path="m@4@5l@4@11@9@11@9@5xe"
   filled="f" stroked="f">
   <v:stroke joinstyle="miter"/>
   <v:formulas>
    <v:f eqn="if lineDrawn pixelLineWidth 0"/>
    <v:f eqn="sum @0 1 0"/>
    <v:f eqn="sum 0 0 @1"/>
    <v:f eqn="prod @2 1 2"/>
    <v:f eqn="prod @3 21600 pixelWidth"/>
    <v:f eqn="prod @3 21600 pixelHeight"/>
    <v:f eqn="sum @0 0 1"/>
    <v:f eqn="prod @6 1 2"/>
    <v:f eqn="prod @7 21600 pixelWidth"/>
    <v:f eqn="sum @8 21600 0"/>
    <v:f eqn="prod @7 21600 pixelHeight"/>
    <v:f eqn="sum @10 21600 0"/>
   </v:formulas>
   <v:path o:extrusionok="f" gradientshapeok="t" o:connecttype="rect"/>
   <o:lock v:ext="edit" aspectratio="t"/>
  </v:shapetype><v:shape id="_x0000_i1025" type="#_x0000_t75" alt="Nightingale"
   style='width:144.75pt;height:53.25pt'>
   <v:imagedata src="./posters-fr_fichiers/image001.jpg" o:href="http://ismir2002.ismir.net/index_fichiers/ngale1.jpg"/>
  </v:shape><![endif]--><![if !vml]><img width=193 height=71
  src="posters-fr_fichiers/image002.jpg" alt=Nightingale v:shapes="_x0000_i1025"><![endif]><span
  lang=EN-GB style='mso-ansi-language:EN-GB'><o:p></o:p></span></p>
  <p class=MsoNormal style='margin-top:30.0pt;tab-stops:14.2pt 108.0pt'><span
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial'><a
  href="index-fr.html">Informations générales</a><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt 108.0pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><a href="calendar-fr.html"><span
  lang=FR style='mso-ansi-language:FR'>Calendrier</span></a></span><span
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial'><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt 108.0pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><a
  href="submissions-fr.html"><span lang=FR
  style='mso-ansi-language:FR'>Directives de soumission</span></a></span><span
  lang=EN-GB style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:
  Arial'> </span><span style='font-size:10.0pt;mso-bidi-font-size:12.0pt;
  font-family:Arial'><o:p></o:p></span></p>
  <p class=MsoHeading7><span style='mso-ansi-language:FR;font-weight:normal'><a
  href="program-fr.html">Programme de la conférence</a><o:p></o:p></span></p>
  <h5 style='margin-left:28.4pt;text-indent:-14.2pt;tab-stops:14.2pt'><span
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:FR;font-weight:normal'><a href="full-program.html">Programme
  en détail</a></span><span style='font-size:10.0pt;mso-bidi-font-size:12.0pt;
  font-family:Arial;mso-fareast-font-family:"Arial Unicode MS";mso-ansi-language:
  FR;font-weight:normal'><o:p></o:p></span></h5>
  <h5 style='margin-left:28.4pt;text-indent:-14.2pt;tab-stops:14.2pt'><span
  lang=EN-GB style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:
  Arial;font-weight:normal'><a href="speakers-fr.html"><span lang=FR
  style='mso-ansi-language:FR'>Invités d'honneur</span></a></span><span
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:FR;font-weight:normal'><o:p></o:p></span></h5>
  <p class=MsoNormal style='margin-left:28.4pt;text-indent:-14.2pt;tab-stops:
  14.2pt'><span lang=EN-GB style='font-size:10.0pt;mso-bidi-font-size:12.0pt;
  font-family:Arial;mso-ansi-language:EN-GB'><a href="metadata-fr.html"><span
  lang=FR style='mso-ansi-language:FR'>Session spéciale Métadonnées</span></a></span><span
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial'><o:p></o:p></span></p>
  <h6><span style='font-weight:normal'><a href="papers-fr.html">Articles
  acceptés</a><o:p></o:p></span></h6>
  <p class=MsoNormal style='margin-left:28.4pt;text-indent:-14.2pt;tab-stops:
  14.2pt'><b><span style='font-size:10.0pt;mso-bidi-font-size:12.0pt;
  font-family:Arial'>Panneaux/articles courts acceptés<o:p></o:p></span></b></p>
  <p class=MsoNormal style='margin-left:28.4pt;text-indent:-14.2pt;tab-stops:
  14.2pt'><span style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:
  Arial'><a href="tutorials-fr.html">Tutoriels</a><o:p></o:p></span></p>
  <p class=MsoNormal style='margin-left:28.4pt;text-indent:-14.2pt;tab-stops:
  14.2pt'><span style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:
  Arial'><a href="panels-fr.html">Tables rondes</a><o:p></o:p></span></p>
  <p class=MsoNormal style='margin-left:28.4pt;text-indent:-14.2pt;tab-stops:
  14.2pt'><span style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:
  Arial'><a href="visit-fr.html">Visite professionnelle</a><o:p></o:p></span></p>
  <p class=MsoNormal style='margin-left:28.4pt;text-indent:-14.2pt;tab-stops:
  14.2pt'><span style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:
  Arial'><a href="social-fr.html">Programme social</a><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt 108.0pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><a
  href="https://www.ircam.fr/ismir2002-register-fr.html"><span lang=FR
  style='mso-ansi-language:FR'>Inscription</span></a></span><span
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial'><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt 108.0pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><a href="venue-fr.html"><span lang=FR
  style='mso-ansi-language:FR'>Site de la conférence</span></a></span><span
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial'><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt 108.0pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><a href="committee-fr.html"><span
  lang=FR style='mso-ansi-language:FR'>Comité d'organisation</span></a></span><span
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial'><o:p></o:p></span></p>
  <p class=MsoNormal style='tab-stops:14.2pt 108.0pt'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial;
  mso-ansi-language:EN-GB'><a
  href="mailing-list-fr.html"><span lang=FR
  style='mso-ansi-language:FR'>Liste de diffusion</span></a></span><span
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial'><o:p></o:p></span></p>
  <h4 style='margin-top:12.0pt'><span lang=EN-GB>Accueil</span></h4>
  <p class=MsoNormal><span lang=EN-GB style='font-size:10.0pt;mso-bidi-font-size:
  12.0pt;font-family:Arial;mso-ansi-language:EN-GB'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></p>
  <p class=MsoNormal align=center style='text-align:center'><span
  style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:Arial'><a
  href="http://www.ircam.fr/"><span style='text-decoration:none;text-underline:
  none'><!--[if gte vml 1]><v:shape id="_x0000_i1026" type="#_x0000_t75"
   href="http://www.ircam.fr/" style='width:87pt;height:51.75pt'
   o:allowoverlap="f" o:button="t">
   <v:fill o:detectmouseclick="t"/>
   <v:imagedata src="./posters-fr_fichiers/image003.wmz" o:althref="./posters-fr_fichiers/image004.pcz"
    o:title=""/>
  </v:shape><![endif]--><![if !vml]><img border=0 width=116 height=69
  src="posters-fr_fichiers/image005.gif" v:shapes="_x0000_i1026"><![endif]></span></a></span><span
  lang=EN-GB style='font-size:10.0pt;mso-bidi-font-size:12.0pt;font-family:
  Arial;mso-ansi-language:EN-GB'><o:p></o:p></span></p>
  <h4 style='margin-top:12.0pt'><span style='mso-ansi-language:FR'>Partenariats
  et subventions<o:p></o:p></span></h4>
  <p class=MsoNormal align=center style='text-align:center'><br>
  <a href="http://www.mairie-paris.fr/"><span style='color:windowtext;
  text-decoration:none;text-underline:none'><!--[if gte vml 1]><v:shape id="_x0000_i1030"
   type="#_x0000_t75" alt="Mairie de Paris" style='width:123.75pt;height:14.25pt'>
   <v:imagedata src="./posters-fr_fichiers/image006.gif" o:href="http://www.mairie-paris.fr/images/commun/logo_mairie_paris.gif"/>
  </v:shape><![endif]--><![if !vml]><img border=0 width=165 height=19
  src="posters-fr_fichiers/image006.gif" alt="Mairie de Paris" v:shapes="_x0000_i1030"><![endif]></span></a><br>
  <br>
  <a href="http://www.nsf.gov/"><span style='color:windowtext;text-decoration:
  none;text-underline:none'><!--[if gte vml 1]><v:shape id="_x0000_i1027"
   type="#_x0000_t75" alt="" style='width:68.25pt;height:75pt'>
   <v:imagedata src="./posters-fr_fichiers/image007.gif" o:href="http://ismir2001.indiana.edu/images/nsflogo1.gif"/>
  </v:shape><![endif]--><![if !vml]><img border=0 width=91 height=100
  src="posters-fr_fichiers/image007.gif" v:shapes="_x0000_i1027"><![endif]></span></a><br>
  <br>
  <a href="http://www.indiana.edu/"><span style='color:windowtext;text-decoration:
  none;text-underline:none'><!--[if gte vml 1]><v:shape id="_x0000_i1028"
   type="#_x0000_t75" alt="IU seal, red on white, small" style='width:68.25pt;
   height:68.25pt' o:ole="">
   <v:imagedata src="./posters-fr_fichiers/image008.wmz" o:title=""/>
  </v:shape><![endif]--><![if !vml]><img border=0 width=91 height=91
  src="posters-fr_fichiers/image009.gif" alt="IU seal, red on white, small"
  v:shapes="_x0000_i1028"><![endif]><!--[if gte mso 9]><xml>
   <o:OLEObject Type="Embed" ProgID="Word.Picture.8" ShapeID="_x0000_i1028"
    DrawAspect="Content" ObjectID="_1093173550">
   </o:OLEObject>
  </xml><![endif]--></span></a><br>
  <br>
  <a href="http://www.cnrs.fr/"><span style='text-decoration:none;text-underline:
  none'><!--[if gte vml 1]><v:shape id="_x0000_i1029" type="#_x0000_t75"
   style='width:98.25pt;height:42pt'>
   <v:imagedata src="./posters-fr_fichiers/image010.gif" o:title="art93-4[1]"/>
  </v:shape><![endif]--><![if !vml]><img border=0 width=131 height=56
  src="posters-fr_fichiers/image011.gif" v:shapes="_x0000_i1029"><![endif]></span></a><span
  lang=EN-GB style='mso-ansi-language:EN-GB'><o:p></o:p></span></p>
  <p class=MsoNormal><![if !supportEmptyParas]>&nbsp;<![endif]><span
  lang=EN-GB style='mso-ansi-language:EN-GB'><o:p></o:p></span></p>
  </td>
  <td width=537 valign=top style='width:402.95pt;border:none;border-bottom:
  solid windowtext .5pt;mso-border-left-alt:solid windowtext .5pt;background:
  #FFFFCC;padding:0cm 3.5pt 0cm 3.5pt'>
  <h1 align=center style='margin-top:0cm;text-align:center'><span
  style='font-size:14.0pt;mso-bidi-font-size:16.0pt'>ISMIR 2002<br>
  3rd International Conference on<br>
  Music Information Retrieval<o:p></o:p></span></h1>
  <h2 align=center style='text-align:center'><span style='font-size:12.0pt;
  mso-bidi-font-size:14.0pt'>IRCAM – Centre Pompidou<br>
  Paris, France<br>
  13-17 octobre 2002<o:p></o:p></span></h2>
  <h3>PANNEAUX/ARTICLES COURTS ACCEPTÉS</h3>
  <p class=MsoBodyTextFirstIndent style='text-align:justify'><span
  style='mso-ansi-language:FR'>Les contributions dans cette catégorie seront
  présentées par leurs auteurs sur des panneaux situés à proximité de la salle
  de conférence plénière. Ils pourront aussi les présenter lors d’interventions
  de 15 minutes (avec projection informatique et diffusion sonore) dans une
  salle choisie à cet effet.<o:p></o:p></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>1.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Masatala Goto (National Institute of Advanced Industrial Science
  and Technology and JST), Hiroki Hashiguchi (Mejiro University), Takuichi
  Nishimura (National Institute of Advanced Industrial Science and Technology)
  and Ryuichi Oka (University of Aizu):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>RWC Music Database:
  Popular, Classical, and Jazz Music <a style='mso-comment-reference:Abstract_1'>Databases
  </a></span></b><![if !supportAnnotations]><a class=msocomanchor id="_anchor_1"
  onmouseover="msoCommentShow('_anchor_1','_com_1')"
  onmouseout="msoCommentHide('_com_1')" href="#_msocom_1" language=JavaScript
  name="_msoanchor_1">[Abstract1]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>2.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Ichiro Fujinaga (Johns Hopkins University) and Jenn Riley (Indiana
  University):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Digital Image Capture of
  Musical <a style='mso-comment-reference:Abstract_2'>Scores </a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_2"
  onmouseover="msoCommentShow('_anchor_2','_com_2')"
  onmouseout="msoCommentHide('_com_2')" href="#_msocom_2" language=JavaScript
  name="_msoanchor_2">[Abstract2]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>3.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Thomas Noll, Jorg Garbers (Technische Universität Berlin), Karin Höthker,
  Christian Spevak (Universität Karlsruhe) and Tillman Weyde (Universität
  Osnabrück):<br>
  <b>Opuscope – </b></span><b><span lang=EN-GB style='font-style:normal'>Towards
  a Corpus-Based Music <a style='mso-comment-reference:Abstract_3'>Repository </a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_3"
  onmouseover="msoCommentShow('_anchor_3','_com_3')"
  onmouseout="msoCommentHide('_com_3')" href="#_msocom_3" language=JavaScript
  name="_msoanchor_3">[Abstract3]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>4.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Beth Logan (Compaq Cambridge Research Laboratory):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Content-Based Playlist
  Generation: Exploratory Experiments </span></b><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt;font-style:
  normal'><a style='mso-comment-reference:Abstract_4'></a><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_4"
  onmouseover="msoCommentShow('_anchor_4','_com_4')"
  onmouseout="msoCommentHide('_com_4')" href="#_msocom_4" language=JavaScript
  name="_msoanchor_4">[Abstract4]</a><![endif]><span style='display:none;
  mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></span><span
  lang=EN-GB style='font-family:"Arial Unicode MS"'> </span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>5.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>J. Stephen Downie (University of Illinois at Urbana-Champaign) and
  Sally Jo Cunningham (University of Waikato):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Towards a Theory of
  Music Information Retrieval Queries: System Design <a style='mso-comment-reference:
  Abstract_5'>Implications </a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_5"
  onmouseover="msoCommentShow('_anchor_5','_com_5')"
  onmouseout="msoCommentHide('_com_5')" href="#_msocom_5" language=JavaScript
  name="_msoanchor_5">[Abstract5]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>6.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Gordon Geekie (Manchester Metropolitan University):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Carnatic </span><span
  lang=EN-GB>Ragas</span></b><b><span lang=EN-GB style='font-style:normal'> as
  Music Information Retrieval <a style='mso-comment-reference:Abstract_6'>Entities
  </a></span></b><![if !supportAnnotations]><a class=msocomanchor id="_anchor_6"
  onmouseover="msoCommentShow('_anchor_6','_com_6')"
  onmouseout="msoCommentHide('_com_6')" href="#_msocom_6" language=JavaScript
  name="_msoanchor_6">[Abstract6]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>7.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Olivier Lartillot (IRCAM – Centre Pompidou):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Integrating Pattern Matching
  into an Analogy-Oriented Pattern Discovery <a style='mso-comment-reference:
  Abstract_7'>Framework </a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_7"
  onmouseover="msoCommentShow('_anchor_7','_com_7')"
  onmouseout="msoCommentHide('_com_7')" href="#_msocom_7" language=JavaScript
  name="_msoanchor_7">[Abstract7]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>8.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Jonathan Foote, Matthew Cooper(FX Palo Alto Laboratories) and
  Unjung Nam (CCRMA):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Audio Retrieval by
  Rhythmic <a style='mso-comment-reference:Abstract_8'>Similarity </a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_8"
  onmouseover="msoCommentShow('_anchor_8','_com_8')"
  onmouseout="msoCommentHide('_com_8')" href="#_msocom_8" language=JavaScript
  name="_msoanchor_8">[Abstract8]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>9.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Toni Heittola and Anssi Klapuri (Tampere University of
  Technology):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Locating Segments with
  Drums in Music <a style='mso-comment-reference:Abstract_9'>Signals </a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_9"
  onmouseover="msoCommentShow('_anchor_9','_com_9')"
  onmouseout="msoCommentHide('_com_9')" href="#_msocom_9" language=JavaScript
  name="_msoanchor_9">[Abstract9]</a><![endif]><span class=MsoCommentReference><span
  lang=EN-GB style='font-size:8.0pt;display:none;mso-hide:all'><span
  style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>10.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>John R. McPherson (University of Waikato):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Introducing Feedback
  into an Optical Music Recognition <a style='mso-comment-reference:Abstract_10'>System
  </a></span></b><![if !supportAnnotations]><a class=msocomanchor
  id="_anchor_10" onmouseover="msoCommentShow('_anchor_10','_com_10')"
  onmouseout="msoCommentHide('_com_10')" href="#_msocom_10"
  language=JavaScript name="_msoanchor_10">[Abstract10]</a><![endif]><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt;display:
  none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>11.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Thomas Mandl and Christa Womser-Hacker (University of Hildesheim):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Learning to cope with
  Diversity in Music <a style='mso-comment-reference:Abstract_11'>Retrieval </a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_11"
  onmouseover="msoCommentShow('_anchor_11','_com_11')"
  onmouseout="msoCommentHide('_com_11')" href="#_msocom_11"
  language=JavaScript name="_msoanchor_11">[Abstract11]</a><![endif]><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt;display:
  none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>12.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Bryan Pardo and William Birmingham (University of Michigan):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Encoding Timing
  Information for Musical Query <a style='mso-comment-reference:Abstract_12'>Matching
  </a></span></b><![if !supportAnnotations]><a class=msocomanchor
  id="_anchor_12" onmouseover="msoCommentShow('_anchor_12','_com_12')"
  onmouseout="msoCommentHide('_com_12')" href="#_msocom_12"
  language=JavaScript name="_msoanchor_12">[Abstract12]</a><![endif]><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt;display:
  none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>13.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Geraint A. Wiggins (City University, London), Kjell Lemström
  (University of Helsinki) and David Meredith (City University, London)<br>
  </span><b><span lang=EN-GB style='font-style:normal'>SIA(M)ESE: An Algorithm
  for Transposition Invariant Polyphonic Content-Based Music <a
  style='mso-comment-reference:Abstract_13'>Retrieval </a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_13"
  onmouseover="msoCommentShow('_anchor_13','_com_13')"
  onmouseout="msoCommentHide('_com_13')" href="#_msocom_13"
  language=JavaScript name="_msoanchor_13">[Abstract13]</a><![endif]><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt;display:
  none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>14.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Emanuelle Pollastri (Università degli Studi di Milano):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Some Considerations
  About Processing Singing Voice for Music <a style='mso-comment-reference:
  Abstract_14'>Retrieval </a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_14"
  onmouseover="msoCommentShow('_anchor_14','_com_14')"
  onmouseout="msoCommentHide('_com_14')" href="#_msocom_14"
  language=JavaScript name="_msoanchor_14">[Abstract14]</a><![endif]><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt;display:
  none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>15.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>David Bainbridge and John R. McPherson (University of Waikato):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Forming a Corpus of
  Voice Queries for Music Information <a style='mso-comment-reference:Abstract_15'>Retrieval
  </a></span></b><![if !supportAnnotations]><a class=msocomanchor
  id="_anchor_15" onmouseover="msoCommentShow('_anchor_15','_com_15')"
  onmouseout="msoCommentHide('_com_15')" href="#_msocom_15"
  language=JavaScript name="_msoanchor_15">[Abstract15]</a><![endif]><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt;display:
  none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>16.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Matija Marolt and Sasa Divjak (University of Ljubljana):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>On detecting repeated
  notes in piano <a style='mso-comment-reference:Abstract_16'>music </a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_16"
  onmouseover="msoCommentShow('_anchor_16','_com_16')"
  onmouseout="msoCommentHide('_com_16')" href="#_msocom_16"
  language=JavaScript name="_msoanchor_16">[Abstract16]</a><![endif]><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt;display:
  none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>17.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Stephan Baumann (German Research Center for AI) and Andreas Klüter
  (sonicson GmbH):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Super-convenience for
  Non-musicians: Querying MP3 and the Semantic <a style='mso-comment-reference:
  Abstract_17'>Web </a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_17"
  onmouseover="msoCommentShow('_anchor_17','_com_17')"
  onmouseout="msoCommentHide('_com_17')" href="#_msocom_17"
  language=JavaScript name="_msoanchor_17">[Abstract17]</a><![endif]><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt;display:
  none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>18.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Pedro Cano, Martin Kaltenbrunner, Fabien Gouyou and Eloi Batlle
  (Universitat Pompeu Fabra):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>On the use of FastMap
  for Audio Retrieval and <a style='mso-comment-reference:Abstract_18'>Browsing
  </a></span></b><![if !supportAnnotations]><a class=msocomanchor
  id="_anchor_18" onmouseover="msoCommentShow('_anchor_18','_com_18')"
  onmouseout="msoCommentHide('_com_18')" href="#_msocom_18"
  language=JavaScript name="_msoanchor_18">[Abstract18]</a><![endif]><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt;display:
  none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>19.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Amar Chaudhary (Creative Advanced Technology Ctr.):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>An Extensible
  Representation for <a style='mso-comment-reference:Abstract_19'>Playlists </a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_19"
  onmouseover="msoCommentShow('_anchor_19','_com_19')"
  onmouseout="msoCommentHide('_com_19')" href="#_msocom_19"
  language=JavaScript name="_msoanchor_19">[Abstract19]</a><![endif]><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt;display:
  none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>20.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Jin Ha Lee, J. Stephen Downie and Allen Renear (University of
  Illinois at Urbana-Champaign):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Representing Traditional
  Korean Music Notation in <a style='mso-comment-reference:Abstract_20'>XML </a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_20"
  onmouseover="msoCommentShow('_anchor_20','_com_20')"
  onmouseout="msoCommentHide('_com_20')" href="#_msocom_20"
  language=JavaScript name="_msoanchor_20">[Abstract20]</a><![endif]><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt;display:
  none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>21.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Yazhong Feng, Yueting Zhuang and Yunhe Pan (Zhejiang University):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Popular Music Retrieval
  by Independent Component <a style='mso-comment-reference:Abstract_21'>Analysis
  </a></span></b><![if !supportAnnotations]><a class=msocomanchor
  id="_anchor_21" onmouseover="msoCommentShow('_anchor_21','_com_21')"
  onmouseout="msoCommentHide('_com_21')" href="#_msocom_21"
  language=JavaScript name="_msoanchor_21">[Abstract21]</a><![endif]><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt;display:
  none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper><![if !supportLists]><span lang=EN-GB>22.<span
  style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;&nbsp; </span></span><![endif]><span
  lang=EN-GB>Timo Sorsa and Katriina Halonen (Nokia Research Center):<br>
  </span><b><span lang=EN-GB style='font-style:normal'>Mobile Melody
  Recognition System<span style="mso-spacerun: yes">  </span>with Voice-Only
  User <a style='mso-comment-reference:Abstract_22'>Interface</a></span></b><![if !supportAnnotations]><a
  class=msocomanchor id="_anchor_22"
  onmouseover="msoCommentShow('_anchor_22','_com_22')"
  onmouseout="msoCommentHide('_com_22')" href="#_msocom_22"
  language=JavaScript name="_msoanchor_22">[Abstract22]</a><![endif]><span
  class=MsoCommentReference><span lang=EN-GB style='font-size:8.0pt;display:
  none;mso-hide:all'><span style='mso-special-character:comment'>&nbsp;</span></span></span></p>
  <p class=Paper style='margin-left:0cm;text-indent:0cm;mso-list:none;
  tab-stops:35.45pt'><![if !supportEmptyParas]>&nbsp;<![endif]><span
  lang=EN-GB><o:p></o:p></span></p>
  </td>
 </tr>
 <tr>
  <td width=740 colspan=2 valign=top style='width:554.85pt;border:none;
  mso-border-top-alt:solid windowtext .5pt;background:#FFFFB3;padding:0cm 3.5pt 0cm 3.5pt'>
  <h1 align=center style='margin-top:0cm;text-align:center'><span lang=EN-GB
  style='font-size:10.0pt;mso-bidi-font-size:16.0pt;mso-ansi-language:EN-GB'>The
  ISMIR 2002 Web pages will be regularly updated<br>
  to include program content and schedule<o:p></o:p></span></h1>
  </td>
 </tr>
</table>

</div>

<p class=MsoNormal><span lang=EN-GB style='mso-ansi-language:EN-GB'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></p>

</div>

<div style='mso-element:comment-list'><![if !supportAnnotations]>

<hr class=msocomoff align=left size=1 width="33%">

<![endif]>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_1" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_1','_com_1')"
onmouseout="msoCommentHide('_com_1')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_1"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_1"
class=msocomoff>[Abstract1]</a><![endif]></span></span></span><span lang=EN-GB
style='mso-ansi-language:EN-GB'>This paper describes the design policy and
specifications of the RWC Music Database, a music database that gives researchers
freedom of common use and research use. Various commonly available databases
have been built in other research fields and have made a significant
contribution to the research in those fields. The field of musical information
processing, however, has lacked a commonly available music database. We
therefore built the RWC Music Database containing four original databases:
Popular Music Database (100 pieces), Royalty-Free Music Database (15 pieces),
Classical Music Database (50 pieces), and Jazz Music Database (50 pieces).
These databases enable researchers to compare and evaluate various methods by
using them as a common benchmark. We also expect that they will accelerate the
progress of various forms of research that use statistical methods. In addition,
researchers can use the databases for research publication and presentation
without copyright restrictions. The music compact discs of these databases are
now available in Japan at a cost equal to only duplication, ship- ping, and
handling charges (virtually for free), and we plan to make them available
outside Japan in 2003. We hope that our database will encourage further
advances in musical information processing research.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_2" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_2','_com_2')"
onmouseout="msoCommentHide('_com_2')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_2"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_2"
class=msocomoff>[Abstract2]</a><![endif]></span></span></span><span lang=EN-GB
style='mso-ansi-language:EN-GB'>The success of information retrieval depends heavily
on the quality of data input into them. Musical scores, as a complex visual
format with small details, are particularly difficult to digitally capture and
deliver well. Virtually all capture decisions should be made with a clear idea
of the purpose of the resulting digital images. Master images must be flexible
enough to fulfill unanticipated future uses as well. In order to provide a
framework for decision-making in musical score capture projects, best practices
for detail and color capture are presented for creating an archival image
containing all relevant data from the print source, based on commonly defined
purposes of digital capture. Options and recommendations for file formats for
archival storage, web delivery and printing of musical materials are presented.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_3" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_3','_com_3')"
onmouseout="msoCommentHide('_com_3')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_3"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_3"
class=msocomoff>[Abstract3]</a><![endif]></span></span></span><span lang=EN-GB
style='mso-ansi-language:EN-GB'>Opuscope is an initiative targeted at sharing
musical corpora and their analyses between researchers. The Opuscope repository
will contain musical corpora of high quality which can be annotated with
hand-made or algorithmic musical analyses. So, analytical results obtained by
others can be used as a starting point for one's own investigations.
Experiments performed on Opuscope corpora can easily be compared to other
approaches, since an unequivocal mechanism for describing a certain corpus will
be provided.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_4" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_4','_com_4')"
onmouseout="msoCommentHide('_com_4')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_4"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_4"
class=msocomoff>[Abstract4]</a><![endif]></span></span></span><span lang=EN-GB
style='mso-bidi-font-size:9.0pt;mso-ansi-language:EN-GB'>We study the use of
content-based techniques to form playlists from a given seed song. Our
techniques use as a basis our previously presented audio similarity measure.
This measure compares songs according to the novelty of their frequency
spectrum and has been shown to have good performance on a non-trivial database.
In this paper we investigate extensions to this basic technique. Specifically,
we study playlists formed by trajectories through the distance space and
playlists formed using automatic relevance feedback. We report results on a
database of over 8000 songs. We find that when information about the songs’
genre is added, improvements over the basic distance measure are obtained,
suggesting both approaches are suitable for incorporating user input or
labeling information if available. </span><span lang=EN-GB style='mso-ansi-language:
EN-GB'><o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_5" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_5','_com_5')"
onmouseout="msoCommentHide('_com_5')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_5"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_5"
class=msocomoff>[Abstract5]</a><![endif]></span></span></span><span lang=EN-GB
style='mso-ansi-language:EN-GB'>This paper analyzes a set of 161 music-related
information requests posted to the rec.music.country.old-time newsgroup. These
postings are categorized by the types of detail used to characterize the
poster's information need, the type of music information requested, the
intended use for the information, and additional social and contextual elements
present in the postings. The results of this analysis suggest that similar
studies of 'native' music information requests can be used to inform the design
of effective, usable music information retrieval interfaces.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_6" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_6','_com_6')"
onmouseout="msoCommentHide('_com_6')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_6"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_6"
class=msocomoff>[Abstract6]</a><![endif]></span></span></span><span lang=EN-GB
style='mso-ansi-language:EN-GB'>This article describes MIR research in Carnatic
music (from southern India), which is characterised by aural transmission and
improvisation. These features have profound implications for the relative
importance and accessibility of different forms of music information available
and for the indigenous attitude towards dissemination of Carnatic music
information. Following Smiraglia`s [2001] methodology, the author identifies
the crucial MIR problems in designing an information resource in this music for
Western users as (i) understanding the indigenous view of the music and (ii)
embedding this understanding in the organisation of the information resource.
The indigenous representation of raga is summarised and illustrated by sample
WAV files and their more detailed analysis, which are downloadable from the
author`s home Web page. The relationship of raga to compositions and
consequently the relationship of improvised performance to cultural and social
meanings is also explained. The author then details the issues arising in the
embedding of this representation in the organisation of an information
resource. Colleagues' views (e.g. on auditory quality and technical
feasibility) and participation (e.g. in tool sharing and experimental digital
audio editing) are sought.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_7" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_7','_com_7')"
onmouseout="msoCommentHide('_com_7')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_7"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_7"
class=msocomoff>[Abstract7]</a><![endif]></span></span></span><span lang=EN-GB
style='mso-ansi-language:EN-GB'>We claim that the core mechanism of a
sufficiently general MIR system should be expressed in symbolic terms. We
defend the idea that music database should be pre-analyzed before being scanned
for MIR queries. We suggest a new vision of automated pattern analysis that
generalizes the multiple viewpoint approach by adding a new paradigm based on
analogy and temporal approach of musical scores. Through a chronological
scanning of the score, analogies are inferred between local relationships --
namely, notes and intervals -- and global structures -- namely, patterns --
whose paradigms are stored inside an abstract pattern tree (APT). Basic
mechanisms for inference of new patterns are described and illustrated. The
same pattern-matching algorithm used for pattern discovery during pre-analysis
of musical works is reused during MIR applications. Such an elastic vision of
music enables a generalized understanding of its plastic expression. This
project, in an early stage, introduces a broader paradigm of automated music
analysis.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_8" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_8','_com_8')"
onmouseout="msoCommentHide('_com_8')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_8"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_8"
class=msocomoff>[Abstract8]</a><![endif]></span></span></span><span lang=EN-GB
style='mso-ansi-language:EN-GB'>We present a methods for characterizing both
the rhythm and tempo of music. We also present ways to quantitatively measure
the rhythmic similarity between two or more works of music. This allows
rhythmically similar works to be retrieved from a large collection. A related
application is to sequence music by rhythmic similarity, thus providing an
automatic &quot;disc jockey&quot; function for musical libraries. Besides
specific analysis and retrieval methods, we present small-scale experiments
that demonstrate ranking and retrieving musical audio by rhythmic similarity.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_9" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_9','_com_9')"
onmouseout="msoCommentHide('_com_9')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_9"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_9"
class=msocomoff>[Abstract9]</a><![endif]></span></span></span><span lang=EN-GB
style='mso-ansi-language:EN-GB'>A system is described which segments musical
signals according to the presence or absence of drum instruments. Two different
yet approximately equally accurate approaches were taken to solve the problem.
The first is based on periodicity detection in the amplitude envelopes of the
signal at subbands. The band-wise periodicity estimates are aggregated into a
summary autocorrelation function, the characteristics of which reveal the
drums. The other mechanism applies straightforward acoustic pattern recognition
approach with mel-frequency cepstrum coefficients as features and a Gaussian
mixture model classifier. The integrated system achieves 88% correct
segmentation over a database of 28 hours of music from different musical
genres. For the both methods, errors occur for borderline cases with soft
percussive-like drum accompaniment, or transient-like instrumentation without
drums.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_10" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_10','_com_10')"
onmouseout="msoCommentHide('_com_10')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_10"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_10"
class=msocomoff>[Abstract10]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>Optical Music Recognition is the
process of converting a graphical representation of music (such as sheet music)
into a symbolic format of use to music software. Music notation is rich in
structural information, and the relative positions of objects can often help to
identify them. When objects are unidentified or mis-identified, many current
systems &quot;coerce&quot; the set of objects into some semantic
representation, for example by modifying the detected durations. This could cause
correctly identified symbols to be modified. The knowledge that the current set
of identified symbols can not be semantically parsed could instead be used to
re-examine some of the symbols before deciding whether or not the
classification is correct. This paper describes work in progress involving the
use of feedback between the various phases of the optical music recognition
process to automatically correct mistakes, such as symbolic classification
errors or mis-detected staff systems.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_11" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_11','_com_11')"
onmouseout="msoCommentHide('_com_11')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_11"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_11"
class=msocomoff>[Abstract11]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>The M-MIMOR approach presented here
makes productive use of the multidimensionality of music retrieval. It
integrates heterogeneous poly-representation into a self adapting system. The
different perspectives of users can be expressed by relevance feedback and
serve as direction for a learning process which ultimately leads to an optimal
solution for a user within a certain context. The paper explores the diversity
within music retrieval stemming from an abundance of approaches for
representing musical objects and searching for similarity. As a result, the
system designer is usually confronted with a large number of arbitrary
decisions. These challenges are discussed within the M-MIMOR framework which
provides an appropriate solution. A fusion with linear combination guarantees
that every perspective is integrated. The weight and therefore the strength of
one perspective is reflected by the weight of the representation scheme or
matching algorithm in the fusion. These weights are adapted according to their
success in previous retrieval tasks.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_12" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_12','_com_12')"
onmouseout="msoCommentHide('_com_12')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_12"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_12"
class=msocomoff>[Abstract12]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>This paper compares the relative
ease of creating a useful quantization of time from linear and log2
representations. The quantization is created by mapping these timing representations
onto different size alphabets and studying the ability of a simple
string-matcher to differentiate between themes in a melodic corpus when
different representations are used. The results indicate that time is better
represented by a logarithmic scale than a linear one. We also compare the
merits of representing timing between events as Inter Onset Intervals (IOIs)
and that taking the ratio of adjacent IOI values, looking at the kind of
information preserved by each and the kinds of variation each minimizes.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_13" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_13','_com_13')"
onmouseout="msoCommentHide('_com_13')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_13"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_13"
class=msocomoff>[Abstract13]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>In this paper, we study
transposition-invariant content-based music retrieval (TI-CBMR) in polyphonic
music. The aim is to find transposition invariant occurrences of a given query
pattern called a template, in a database of polyphonic music called a dataset.
Between the musical events (represented by points) in the dataset that have
been found to match points in the template, there may be any finite number of
other intervening musical events. For this task, we introduce an algorithm,
called SIA(M)ESE, which is based on the SIA pattern induction algorithm. The
algorithm is first introduced in abstract mathematical form, then we show how
we have implemented it using sophisticated techniques and equipped it with appropriate
heuristics. The resulting efficient algorithm has a worst case running time of <i>O(mn
</i>log(<i>mn</i>)), where <i>m </i>and <i>n </i>are the size of the template
and the dataset, respectively. Moreover, the algorithm is generalizable to any
arbitrary, multidimensional translation invariant pattern matching problem,
where the events considered can be represented by points in a multidimensional
dataset.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_14" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_14','_com_14')"
onmouseout="msoCommentHide('_com_14')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_14"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_14"
class=msocomoff>[Abstract14]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>The audio processing and
post-processing of singing hold a fundamental role in the context of
query-by-humming applications. Through the analysis of a sung query, we should
perform some kind of meta-information extraction and this topic deserves the
interest of the present paper. Some considerations are presented aiming to give
a systematic view to a number of issues related to the transcription of singing
into music. A critical review of previous approaches and findings is followed
with novel experimental results. Starting from the similarities between speech
sounds and sung notes, the peculiar facets of singing voices are introduced and
analyzed in accordance with three different directions: extraction of a
microintonation contour (or pitch contour at frame level), note estimation and
study of singing accuracy. A segmentation algorithm has been developed
combining the Spectral Flatness Measure with pitch and envelope information. A
practical implementation for smoothing raw output from pitch tracking and a
rule-based schema for reducing the pitch contour to a sequence of note-duration
pairs are illustrated. Finally, we report an experiment on the deviations from
pure tone intonation in performances of untrained singers.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_15" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_15','_com_15')"
onmouseout="msoCommentHide('_com_15')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_15"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_15"
class=msocomoff>[Abstract15]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>The use of audio queries for
searching multimedia content has increased rapidly with the rise of music
information retrieval; there are now many Internet-accessible systems that take
audio queries as input. However, testing the robustness of such a system can be
a large part of the development process. A corpus of audio queries would aid
researchers in the development of both audio signal processing techniques and
audio query systems. Such a corpus would also be of use for making empirical
comparisons between different systems and methods. We propose the creation of a
set of audio queries taken from attendees of the ISMIR 2002 Conference that
would be made readily available to MIR researchers.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_16" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_16','_com_16')"
onmouseout="msoCommentHide('_com_16')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_16"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_16"
class=msocomoff>[Abstract16]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>One of the problems encountered in
music transcription is to produce an algorithm that detects whether a note
should be repeated, when a new onset is found during its duration, or not; with
other words whether two or more shorter notes should be produced instead of a
single longer note. The paper describes our approach to solving this problem,
implemented within our system for transcription of piano music. The approach is
based on a multilayer perceptron neural network, trained to recognize repeated
notes. We compare this method to a more naive method that tracks the amplitude
of the first partial of each note and also present performance statistics of
our system on transcriptions of several real piano recordings.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_17" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_17','_com_17')"
onmouseout="msoCommentHide('_com_17')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_17"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_17"
class=msocomoff>[Abstract17]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>Electronic music distribution, the
internet success of MP3 and the actual activities concerning the semantic web
of music require for convenient music information retrieval, resp.
question-answering systems. In this paper we will give an overview about the
concepts behind our &quot;super-convenience&quot; approach for MIR. By using
natural language as input for human-oriented queries to large-scale music
collections we were able to address the needs of non-musicians. The entire
system is applicable for future semantic web services, existing music web-sites
and future electronic devices such as cd-chargers for cars, or PDAs. It is a
full-fledged architecture combining state-of-the-art approaches from different
research disciplines. We customized in a cross-discipline approach techniques
from natural language understanding phonetic matching, automatic analysis of
audio for meta tag construction, content-based classification and music
ontologies as a backbone for the representation of musical knowledge. Beside
the basic framework we present a novel idea to incorporate the processing of
lyrics based on standard information retrieval methods, i.e the vector space
model. This work has been performed at the German Research Center for AI and
the authors spin-off company -- sonicson -- specialized in music web services.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_18" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_18','_com_18')"
onmouseout="msoCommentHide('_com_18')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_18"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_18"
class=msocomoff>[Abstract18]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>In this article, a heuristic version
of Multidimensional Scaling named FastMap, is used for audio retrieval and
browsing. FastMap, like MDS, maps objects into an Euclidean space, such that
similarities are preserved. In addition of being more efficient than MDS it
allows query-by-example type of query, which makes it suitable for a content-based
retrieval purposes.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_19" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_19','_com_19')"
onmouseout="msoCommentHide('_com_19')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_19"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_19"
class=msocomoff>[Abstract19]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>The increasing availability of
digital music has created a greater need for methods to organize large
collections of music. The eXtensible PlayList (XPL) representation allows users
to express playlists with varying degrees of specificity. XPL handles
references to exact files or URLs as well as rules for selecting content based
on metadata constraints. XPL also allows the transitions between tracks in a
playlist to be specified. This paper describes the features of XPL, a system
for rendering XPL specifications and use of an advanced XPL renderer in an
existing application.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_20" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_20','_com_20')"
onmouseout="msoCommentHide('_com_20')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_20"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_20"
class=msocomoff>[Abstract20]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>Current XML encoding systems for
music focus almost exclusively on western music from the 17th century onwards,
and on the western notation system. In order to ensure that music information
retrieval (MIR) systems have full theoretical generality, and wide practical
application, we have begun a project to explore the representation, in XML, of
a genre of traditional Korean music which has a distinctive notation system
(Chôngganbo). Our project takes seriously the specific notational expression of
musical intention and intends to ultimately contribute to the analysis of
theoretical issues in music representation, as well as to the improvement of
methods for representing Korean music specifically. The present paper is an
introduction to the music and its notation, and to our exploratory XML
representation system.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_21" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_21','_com_21')"
onmouseout="msoCommentHide('_com_21')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_21"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_21"
class=msocomoff>[Abstract21]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>Singing is the characteristic vocal
part in popular music, retrieval by singing with lyrics is a natural way for
popular music. Unlike some music retrieval systems, which used melody contour
to represent music and string matching to retrieval music, we match acoustic singing
input directly with vocal part of popular music, it seems very difficult to
exactly matching of them, while, they are represented by self-similarity
sequence to eliminate error propagation. Our approach deals with raw audio
music in WAV, independent Component Analysis (ICA) is employed to separate
singing from the accompaniment, we use AbstractCCs as the features to calculate
self-similarity sequence, the weights of recurrent neural network are used as
indices on music database, retrieval list is generated by correlation degree.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

<div style='mso-element:comment'><![if !supportAnnotations]>

<div id="_com_22" class=msocomtxt language=JavaScript
onmouseover="msoCommentShow('_anchor_22','_com_22')"
onmouseout="msoCommentHide('_com_22')"><![endif]><span style='mso-comment-author:
"Michael Fingerhut"'><![if !supportAnnotations]><a name="_msocom_22"></a><![endif]></span>

<p class=MsoCommentText style='text-align:justify;text-justify:inter-ideograph'><!--[if supportFields]><span
style='mso-element:field-begin'></span><span lang=EN-GB style='mso-ansi-language:
EN-GB'>PAGE \# &quot;'Page&nbsp;: '#'<br>
'&quot;</span><span class=MsoCommentReference><span lang=EN-GB
style='font-size:8.0pt;mso-ansi-language:EN-GB'><span style="mso-spacerun:
yes">  </span></span></span><![endif]--><!--[if supportFields]><span
style='mso-element:field-end'></span><![endif]--><span
class=MsoCommentReference><span style='font-size:8.0pt'><span style='mso-special-character:
comment'>&nbsp;<![if !supportAnnotations]><a href="#_msoanchor_22"
class=msocomoff>[Abstract22]</a><![endif]></span></span></span><span
lang=EN-GB style='mso-ansi-language:EN-GB'>A melody recognition system with a
voice-only user interface is presented in this paper. By integrating speech
recognition and music recognition technology we have built an end-to-end melody
recognition system that allows voice controlled melodic queries and melody
generation using a dial-in service with a mobile phone. In this paper we
present the system behind the service, report user evaluation results and
consider the strengths and weaknesses of such service.<o:p></o:p></span></p>

<![if !supportAnnotations]></div>

<![endif]></div>

</div>

</body>


<!-- Mirrored from ismir2002.ismir.net/posters-fr.html by HTTrack Website Copier/3.x [XR&CO'2013], Thu, 23 Jan 2014 11:32:13 GMT -->
</html>
